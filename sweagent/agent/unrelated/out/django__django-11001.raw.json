{
  "instance_id": "django__django-11001",
  "variant": "oracle",
  "split": "test",
  "meta": {
    "instance_id": "django__django-11001",
    "repo": "django/django",
    "base_commit": "ef082ebb84f00e38af4e8880d04e8365c2766d34",
    "patch": "<patch>\ndiff --git a/django/db/models/sql/compiler.py b/django/db/models/sql/compiler.py\n--- a/django/db/models/sql/compiler.py\n+++ b/django/db/models/sql/compiler.py\n@@ -32,7 +32,8 @@ def __init__(self, query, connection, using):\n         self.select = None\n         self.annotation_col_map = None\n         self.klass_info = None\n-        self.ordering_parts = re.compile(r'(.*)\\s(ASC|DESC)(.*)')\n+        # Multiline ordering SQL clause may appear from RawSQL.\n+        self.ordering_parts = re.compile(r'^(.*)\\s(ASC|DESC)(.*)', re.MULTILINE | re.DOTALL)\n         self._meta_ordering = None\n \n     def setup_query(self):\n\n</patch>",
    "test_patch": "diff --git a/tests/expressions/tests.py b/tests/expressions/tests.py\n--- a/tests/expressions/tests.py\n+++ b/tests/expressions/tests.py\n@@ -384,6 +384,29 @@ def test_order_by_exists(self):\n         )\n         self.assertSequenceEqual(mustermanns_by_seniority, [self.max, mary])\n \n+    def test_order_by_multiline_sql(self):\n+        raw_order_by = (\n+            RawSQL('''\n+                CASE WHEN num_employees > 1000\n+                     THEN num_chairs\n+                     ELSE 0 END\n+            ''', []).desc(),\n+            RawSQL('''\n+                CASE WHEN num_chairs > 1\n+                     THEN 1\n+                     ELSE 0 END\n+            ''', []).asc()\n+        )\n+        for qs in (\n+            Company.objects.all(),\n+            Company.objects.distinct(),\n+        ):\n+            with self.subTest(qs=qs):\n+                self.assertSequenceEqual(\n+                    qs.order_by(*raw_order_by),\n+                    [self.example_inc, self.gmbh, self.foobar_ltd],\n+                )\n+\n     def test_outerref(self):\n         inner = Company.objects.filter(point_of_contact=OuterRef('pk'))\n         msg = (\n",
    "created_at": "2019-02-17T13:02:09Z",
    "version": "3.0",
    "FAIL_TO_PASS": "[\"test_order_by_multiline_sql (expressions.tests.BasicExpressionsTests)\", \"test_order_of_operations (expressions.tests.BasicExpressionsTests)\"]",
    "PASS_TO_PASS": "[\"test_deconstruct (expressions.tests.FTests)\", \"test_deepcopy (expressions.tests.FTests)\", \"test_equal (expressions.tests.FTests)\", \"test_hash (expressions.tests.FTests)\", \"test_not_equal_Value (expressions.tests.FTests)\", \"test_and (expressions.tests.CombinableTests)\", \"test_negation (expressions.tests.CombinableTests)\", \"test_or (expressions.tests.CombinableTests)\", \"test_reversed_and (expressions.tests.CombinableTests)\", \"test_reversed_or (expressions.tests.CombinableTests)\", \"test_aggregates (expressions.tests.ReprTests)\", \"test_distinct_aggregates (expressions.tests.ReprTests)\", \"test_expressions (expressions.tests.ReprTests)\", \"test_filtered_aggregates (expressions.tests.ReprTests)\", \"test_functions (expressions.tests.ReprTests)\", \"test_equal (expressions.tests.SimpleExpressionTests)\", \"test_hash (expressions.tests.SimpleExpressionTests)\", \"test_month_aggregation (expressions.tests.FieldTransformTests)\", \"test_multiple_transforms_in_values (expressions.tests.FieldTransformTests)\", \"test_transform_in_values (expressions.tests.FieldTransformTests)\", \"test_deconstruct (expressions.tests.ValueTests)\", \"test_deconstruct_output_field (expressions.tests.ValueTests)\", \"test_equal (expressions.tests.ValueTests)\", \"test_equal_output_field (expressions.tests.ValueTests)\", \"test_hash (expressions.tests.ValueTests)\", \"test_raise_empty_expressionlist (expressions.tests.ValueTests)\", \"test_update_TimeField_using_Value (expressions.tests.ValueTests)\", \"test_update_UUIDField_using_Value (expressions.tests.ValueTests)\", \"test_complex_expressions (expressions.tests.ExpressionsNumericTests)\", \"test_fill_with_value_from_same_object (expressions.tests.ExpressionsNumericTests)\", \"test_filter_not_equals_other_field (expressions.tests.ExpressionsNumericTests)\", \"test_increment_value (expressions.tests.ExpressionsNumericTests)\", \"test_F_reuse (expressions.tests.ExpressionsTests)\", \"test_insensitive_patterns_escape (expressions.tests.ExpressionsTests)\", \"test_patterns_escape (expressions.tests.ExpressionsTests)\", \"test_complex_expressions_do_not_introduce_sql_injection_via_untrusted_string_inclusion (expressions.tests.IterableLookupInnerExpressionsTests)\", \"test_expressions_in_lookups_join_choice (expressions.tests.IterableLookupInnerExpressionsTests)\", \"test_in_lookup_allows_F_expressions_and_expressions_for_datetimes (expressions.tests.IterableLookupInnerExpressionsTests)\", \"test_in_lookup_allows_F_expressions_and_expressions_for_integers (expressions.tests.IterableLookupInnerExpressionsTests)\", \"test_range_lookup_allows_F_expressions_and_expressions_for_integers (expressions.tests.IterableLookupInnerExpressionsTests)\", \"test_lefthand_addition (expressions.tests.ExpressionOperatorTests)\", \"test_lefthand_bitwise_and (expressions.tests.ExpressionOperatorTests)\", \"test_lefthand_bitwise_left_shift_operator (expressions.tests.ExpressionOperatorTests)\", \"test_lefthand_bitwise_or (expressions.tests.ExpressionOperatorTests)\", \"test_lefthand_bitwise_right_shift_operator (expressions.tests.ExpressionOperatorTests)\", \"test_lefthand_division (expressions.tests.ExpressionOperatorTests)\", \"test_lefthand_modulo (expressions.tests.ExpressionOperatorTests)\", \"test_lefthand_multiplication (expressions.tests.ExpressionOperatorTests)\", \"test_lefthand_power (expressions.tests.ExpressionOperatorTests)\", \"test_lefthand_subtraction (expressions.tests.ExpressionOperatorTests)\", \"test_right_hand_addition (expressions.tests.ExpressionOperatorTests)\", \"test_right_hand_division (expressions.tests.ExpressionOperatorTests)\", \"test_right_hand_modulo (expressions.tests.ExpressionOperatorTests)\", \"test_right_hand_multiplication (expressions.tests.ExpressionOperatorTests)\", \"test_right_hand_subtraction (expressions.tests.ExpressionOperatorTests)\", \"test_righthand_power (expressions.tests.ExpressionOperatorTests)\", \"test_aggregate_subquery_annotation (expressions.tests.BasicExpressionsTests)\", \"test_annotate_values_aggregate (expressions.tests.BasicExpressionsTests)\", \"test_annotate_values_count (expressions.tests.BasicExpressionsTests)\", \"test_annotate_values_filter (expressions.tests.BasicExpressionsTests)\", \"test_annotation_with_outerref (expressions.tests.BasicExpressionsTests)\", \"test_annotations_within_subquery (expressions.tests.BasicExpressionsTests)\", \"test_arithmetic (expressions.tests.BasicExpressionsTests)\", \"test_exist_single_field_output_field (expressions.tests.BasicExpressionsTests)\", \"test_explicit_output_field (expressions.tests.BasicExpressionsTests)\", \"test_filter_inter_attribute (expressions.tests.BasicExpressionsTests)\", \"test_filter_with_join (expressions.tests.BasicExpressionsTests)\", \"test_filtering_on_annotate_that_uses_q (expressions.tests.BasicExpressionsTests)\", \"test_in_subquery (expressions.tests.BasicExpressionsTests)\", \"test_incorrect_field_in_F_expression (expressions.tests.BasicExpressionsTests)\", \"test_incorrect_joined_field_in_F_expression (expressions.tests.BasicExpressionsTests)\", \"test_nested_subquery (expressions.tests.BasicExpressionsTests)\", \"test_nested_subquery_outer_ref_2 (expressions.tests.BasicExpressionsTests)\", \"test_nested_subquery_outer_ref_with_autofield (expressions.tests.BasicExpressionsTests)\", \"test_new_object_create (expressions.tests.BasicExpressionsTests)\", \"test_new_object_save (expressions.tests.BasicExpressionsTests)\", \"test_object_create_with_aggregate (expressions.tests.BasicExpressionsTests)\", \"test_object_update (expressions.tests.BasicExpressionsTests)\", \"test_object_update_fk (expressions.tests.BasicExpressionsTests)\", \"test_object_update_unsaved_objects (expressions.tests.BasicExpressionsTests)\", \"test_order_by_exists (expressions.tests.BasicExpressionsTests)\", \"test_outerref (expressions.tests.BasicExpressionsTests)\", \"test_outerref_mixed_case_table_name (expressions.tests.BasicExpressionsTests)\", \"test_outerref_with_operator (expressions.tests.BasicExpressionsTests)\", \"test_parenthesis_priority (expressions.tests.BasicExpressionsTests)\", \"test_pickle_expression (expressions.tests.BasicExpressionsTests)\", \"test_subquery (expressions.tests.BasicExpressionsTests)\", \"test_subquery_filter_by_aggregate (expressions.tests.BasicExpressionsTests)\", \"test_subquery_references_joined_table_twice (expressions.tests.BasicExpressionsTests)\", \"test_ticket_11722_iexact_lookup (expressions.tests.BasicExpressionsTests)\", \"test_ticket_16731_startswith_lookup (expressions.tests.BasicExpressionsTests)\", \"test_ticket_18375_chained_filters (expressions.tests.BasicExpressionsTests)\", \"test_ticket_18375_join_reuse (expressions.tests.BasicExpressionsTests)\", \"test_ticket_18375_kwarg_ordering (expressions.tests.BasicExpressionsTests)\", \"test_ticket_18375_kwarg_ordering_2 (expressions.tests.BasicExpressionsTests)\", \"test_update (expressions.tests.BasicExpressionsTests)\", \"test_update_inherited_field_value (expressions.tests.BasicExpressionsTests)\", \"test_update_with_fk (expressions.tests.BasicExpressionsTests)\", \"test_update_with_none (expressions.tests.BasicExpressionsTests)\", \"test_uuid_pk_subquery (expressions.tests.BasicExpressionsTests)\", \"test_date_comparison (expressions.tests.FTimeDeltaTests)\", \"test_date_minus_duration (expressions.tests.FTimeDeltaTests)\", \"test_date_subtraction (expressions.tests.FTimeDeltaTests)\", \"test_datetime_subtraction (expressions.tests.FTimeDeltaTests)\", \"test_datetime_subtraction_microseconds (expressions.tests.FTimeDeltaTests)\", \"test_delta_add (expressions.tests.FTimeDeltaTests)\", \"test_delta_subtract (expressions.tests.FTimeDeltaTests)\", \"test_delta_update (expressions.tests.FTimeDeltaTests)\", \"test_duration_with_datetime (expressions.tests.FTimeDeltaTests)\", \"test_duration_with_datetime_microseconds (expressions.tests.FTimeDeltaTests)\", \"test_durationfield_add (expressions.tests.FTimeDeltaTests)\", \"test_exclude (expressions.tests.FTimeDeltaTests)\", \"test_invalid_operator (expressions.tests.FTimeDeltaTests)\", \"test_mixed_comparisons2 (expressions.tests.FTimeDeltaTests)\", \"test_multiple_query_compilation (expressions.tests.FTimeDeltaTests)\", \"test_negative_timedelta_update (expressions.tests.FTimeDeltaTests)\", \"test_query_clone (expressions.tests.FTimeDeltaTests)\", \"test_time_subtraction (expressions.tests.FTimeDeltaTests)\"]"
  },
  "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nIncorrect removal of order_by clause created as multiline RawSQL\nDescription\n\t\nHi.\nThe SQLCompiler is ripping off one of my \"order by\" clause, because he \"thinks\" the clause was already \"seen\" (in SQLCompiler.get_order_by()). I'm using expressions written as multiline RawSQLs, which are similar but not the same. \nThe bug is located in SQLCompiler.get_order_by(), somewhere around line computing part of SQL query without ordering:\nwithout_ordering = self.ordering_parts.search(sql).group(1)\nThe sql variable contains multiline sql. As a result, the self.ordering_parts regular expression is returning just a line containing ASC or DESC words. This line is added to seen set, and because my raw queries have identical last lines, only the first clasue is returing from SQLCompiler.get_order_by().\nAs a quick/temporal fix I can suggest making sql variable clean of newline characters, like this:\nsql_oneline = ' '.join(sql.split('\\n'))\nwithout_ordering = self.ordering_parts.search(sql_oneline).group(1)\nNote: beware of unicode (Py2.x u'') and EOL dragons (\\r).\nExample of my query:\n\treturn MyModel.objects.all().order_by(\n\t\tRawSQL('''\n\t\t\tcase when status in ('accepted', 'verification')\n\t\t\t\t then 2 else 1 end''', []).desc(),\n\t\tRawSQL('''\n\t\t\tcase when status in ('accepted', 'verification')\n\t\t\t\t then (accepted_datetime, preferred_datetime)\n\t\t\t\t else null end''', []).asc(),\n\t\tRawSQL('''\n\t\t\tcase when status not in ('accepted', 'verification')\n\t\t\t\t then (accepted_datetime, preferred_datetime, created_at)\n\t\t\t\t else null end''', []).desc())\nThe ordering_parts.search is returing accordingly:\n'\t\t\t\t then 2 else 1 end)'\n'\t\t\t\t else null end'\n'\t\t\t\t else null end'\nSecond RawSQL with a\t\t\t\t else null end part is removed from query.\nThe fun thing is that the issue can be solved by workaround by adding a space or any other char to the last line. \nSo in case of RawSQL I can just say, that current implementation of avoiding duplicates in order by clause works only for special/rare cases (or does not work in all cases). \nThe bug filed here is about wrong identification of duplicates (because it compares only last line of SQL passed to order by clause).\nHope my notes will help you fixing the issue. Sorry for my english.\n\n</issue>\n<code>\n[start of README.rst]\n1 Django is a high-level Python Web framework that encourages rapid development\n2 and clean, pragmatic design. Thanks for checking it out.\n3 \n4 All documentation is in the \"``docs``\" directory and online at\n5 https://docs.djangoproject.com/en/stable/. If you're just getting started,\n6 here's how we recommend you read the docs:\n7 \n8 * First, read ``docs/intro/install.txt`` for instructions on installing Django.\n9 \n10 * Next, work through the tutorials in order (``docs/intro/tutorial01.txt``,\n11   ``docs/intro/tutorial02.txt``, etc.).\n12 \n13 * If you want to set up an actual deployment server, read\n14   ``docs/howto/deployment/index.txt`` for instructions.\n15 \n16 * You'll probably want to read through the topical guides (in ``docs/topics``)\n17   next; from there you can jump to the HOWTOs (in ``docs/howto``) for specific\n18   problems, and check out the reference (``docs/ref``) for gory details.\n19 \n20 * See ``docs/README`` for instructions on building an HTML version of the docs.\n21 \n22 Docs are updated rigorously. If you find any problems in the docs, or think\n23 they should be clarified in any way, please take 30 seconds to fill out a\n24 ticket here: https://code.djangoproject.com/newticket\n25 \n26 To get more help:\n27 \n28 * Join the ``#django`` channel on irc.freenode.net. Lots of helpful people hang\n29   out there. See https://en.wikipedia.org/wiki/Wikipedia:IRC/Tutorial if you're\n30   new to IRC.\n31 \n32 * Join the django-users mailing list, or read the archives, at\n33   https://groups.google.com/group/django-users.\n34 \n35 To contribute to Django:\n36 \n37 * Check out https://docs.djangoproject.com/en/dev/internals/contributing/ for\n38   information about getting involved.\n39 \n40 To run Django's test suite:\n41 \n42 * Follow the instructions in the \"Unit tests\" section of\n43   ``docs/internals/contributing/writing-code/unit-tests.txt``, published online at\n44   https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/unit-tests/#running-the-unit-tests\n45 \n[end of README.rst]\n[start of django/db/models/sql/compiler.py]\n1 import collections\n2 import re\n3 import warnings\n4 from itertools import chain\n5 \n6 from django.core.exceptions import EmptyResultSet, FieldError\n7 from django.db.models.constants import LOOKUP_SEP\n8 from django.db.models.expressions import OrderBy, Random, RawSQL, Ref\n9 from django.db.models.query_utils import QueryWrapper, select_related_descend\n10 from django.db.models.sql.constants import (\n11     CURSOR, GET_ITERATOR_CHUNK_SIZE, MULTI, NO_RESULTS, ORDER_DIR, SINGLE,\n12 )\n13 from django.db.models.sql.query import Query, get_order_dir\n14 from django.db.transaction import TransactionManagementError\n15 from django.db.utils import DatabaseError, NotSupportedError\n16 from django.utils.deprecation import RemovedInDjango31Warning\n17 from django.utils.hashable import make_hashable\n18 \n19 FORCE = object()\n20 \n21 \n22 class SQLCompiler:\n23     def __init__(self, query, connection, using):\n24         self.query = query\n25         self.connection = connection\n26         self.using = using\n27         self.quote_cache = {'*': '*'}\n28         # The select, klass_info, and annotations are needed by QuerySet.iterator()\n29         # these are set as a side-effect of executing the query. Note that we calculate\n30         # separately a list of extra select columns needed for grammatical correctness\n31         # of the query, but these columns are not included in self.select.\n32         self.select = None\n33         self.annotation_col_map = None\n34         self.klass_info = None\n35         self.ordering_parts = re.compile(r'(.*)\\s(ASC|DESC)(.*)')\n36         self._meta_ordering = None\n37 \n38     def setup_query(self):\n39         if all(self.query.alias_refcount[a] == 0 for a in self.query.alias_map):\n40             self.query.get_initial_alias()\n41         self.select, self.klass_info, self.annotation_col_map = self.get_select()\n42         self.col_count = len(self.select)\n43 \n44     def pre_sql_setup(self):\n45         \"\"\"\n46         Do any necessary class setup immediately prior to producing SQL. This\n47         is for things that can't necessarily be done in __init__ because we\n48         might not have all the pieces in place at that time.\n49         \"\"\"\n50         self.setup_query()\n51         order_by = self.get_order_by()\n52         self.where, self.having = self.query.where.split_having()\n53         extra_select = self.get_extra_select(order_by, self.select)\n54         self.has_extra_select = bool(extra_select)\n55         group_by = self.get_group_by(self.select + extra_select, order_by)\n56         return extra_select, order_by, group_by\n57 \n58     def get_group_by(self, select, order_by):\n59         \"\"\"\n60         Return a list of 2-tuples of form (sql, params).\n61 \n62         The logic of what exactly the GROUP BY clause contains is hard\n63         to describe in other words than \"if it passes the test suite,\n64         then it is correct\".\n65         \"\"\"\n66         # Some examples:\n67         #     SomeModel.objects.annotate(Count('somecol'))\n68         #     GROUP BY: all fields of the model\n69         #\n70         #    SomeModel.objects.values('name').annotate(Count('somecol'))\n71         #    GROUP BY: name\n72         #\n73         #    SomeModel.objects.annotate(Count('somecol')).values('name')\n74         #    GROUP BY: all cols of the model\n75         #\n76         #    SomeModel.objects.values('name', 'pk').annotate(Count('somecol')).values('pk')\n77         #    GROUP BY: name, pk\n78         #\n79         #    SomeModel.objects.values('name').annotate(Count('somecol')).values('pk')\n80         #    GROUP BY: name, pk\n81         #\n82         # In fact, the self.query.group_by is the minimal set to GROUP BY. It\n83         # can't be ever restricted to a smaller set, but additional columns in\n84         # HAVING, ORDER BY, and SELECT clauses are added to it. Unfortunately\n85         # the end result is that it is impossible to force the query to have\n86         # a chosen GROUP BY clause - you can almost do this by using the form:\n87         #     .values(*wanted_cols).annotate(AnAggregate())\n88         # but any later annotations, extra selects, values calls that\n89         # refer some column outside of the wanted_cols, order_by, or even\n90         # filter calls can alter the GROUP BY clause.\n91 \n92         # The query.group_by is either None (no GROUP BY at all), True\n93         # (group by select fields), or a list of expressions to be added\n94         # to the group by.\n95         if self.query.group_by is None:\n96             return []\n97         expressions = []\n98         if self.query.group_by is not True:\n99             # If the group by is set to a list (by .values() call most likely),\n100             # then we need to add everything in it to the GROUP BY clause.\n101             # Backwards compatibility hack for setting query.group_by. Remove\n102             # when  we have public API way of forcing the GROUP BY clause.\n103             # Converts string references to expressions.\n104             for expr in self.query.group_by:\n105                 if not hasattr(expr, 'as_sql'):\n106                     expressions.append(self.query.resolve_ref(expr))\n107                 else:\n108                     expressions.append(expr)\n109         # Note that even if the group_by is set, it is only the minimal\n110         # set to group by. So, we need to add cols in select, order_by, and\n111         # having into the select in any case.\n112         for expr, _, _ in select:\n113             cols = expr.get_group_by_cols()\n114             for col in cols:\n115                 expressions.append(col)\n116         for expr, (sql, params, is_ref) in order_by:\n117             # Skip References to the select clause, as all expressions in the\n118             # select clause are already part of the group by.\n119             if not expr.contains_aggregate and not is_ref:\n120                 expressions.extend(expr.get_source_expressions())\n121         having_group_by = self.having.get_group_by_cols() if self.having else ()\n122         for expr in having_group_by:\n123             expressions.append(expr)\n124         result = []\n125         seen = set()\n126         expressions = self.collapse_group_by(expressions, having_group_by)\n127 \n128         for expr in expressions:\n129             sql, params = self.compile(expr)\n130             params_hash = make_hashable(params)\n131             if (sql, params_hash) not in seen:\n132                 result.append((sql, params))\n133                 seen.add((sql, params_hash))\n134         return result\n135 \n136     def collapse_group_by(self, expressions, having):\n137         # If the DB can group by primary key, then group by the primary key of\n138         # query's main model. Note that for PostgreSQL the GROUP BY clause must\n139         # include the primary key of every table, but for MySQL it is enough to\n140         # have the main table's primary key.\n141         if self.connection.features.allows_group_by_pk:\n142             # Determine if the main model's primary key is in the query.\n143             pk = None\n144             for expr in expressions:\n145                 # Is this a reference to query's base table primary key? If the\n146                 # expression isn't a Col-like, then skip the expression.\n147                 if (getattr(expr, 'target', None) == self.query.model._meta.pk and\n148                         getattr(expr, 'alias', None) == self.query.base_table):\n149                     pk = expr\n150                     break\n151             # If the main model's primary key is in the query, group by that\n152             # field, HAVING expressions, and expressions associated with tables\n153             # that don't have a primary key included in the grouped columns.\n154             if pk:\n155                 pk_aliases = {\n156                     expr.alias for expr in expressions\n157                     if hasattr(expr, 'target') and expr.target.primary_key\n158                 }\n159                 expressions = [pk] + [\n160                     expr for expr in expressions\n161                     if expr in having or (\n162                         getattr(expr, 'alias', None) is not None and expr.alias not in pk_aliases\n163                     )\n164                 ]\n165         elif self.connection.features.allows_group_by_selected_pks:\n166             # Filter out all expressions associated with a table's primary key\n167             # present in the grouped columns. This is done by identifying all\n168             # tables that have their primary key included in the grouped\n169             # columns and removing non-primary key columns referring to them.\n170             # Unmanaged models are excluded because they could be representing\n171             # database views on which the optimization might not be allowed.\n172             pks = {\n173                 expr for expr in expressions\n174                 if hasattr(expr, 'target') and expr.target.primary_key and expr.target.model._meta.managed\n175             }\n176             aliases = {expr.alias for expr in pks}\n177             expressions = [\n178                 expr for expr in expressions if expr in pks or getattr(expr, 'alias', None) not in aliases\n179             ]\n180         return expressions\n181 \n182     def get_select(self):\n183         \"\"\"\n184         Return three values:\n185         - a list of 3-tuples of (expression, (sql, params), alias)\n186         - a klass_info structure,\n187         - a dictionary of annotations\n188 \n189         The (sql, params) is what the expression will produce, and alias is the\n190         \"AS alias\" for the column (possibly None).\n191 \n192         The klass_info structure contains the following information:\n193         - The base model of the query.\n194         - Which columns for that model are present in the query (by\n195           position of the select clause).\n196         - related_klass_infos: [f, klass_info] to descent into\n197 \n198         The annotations is a dictionary of {'attname': column position} values.\n199         \"\"\"\n200         select = []\n201         klass_info = None\n202         annotations = {}\n203         select_idx = 0\n204         for alias, (sql, params) in self.query.extra_select.items():\n205             annotations[alias] = select_idx\n206             select.append((RawSQL(sql, params), alias))\n207             select_idx += 1\n208         assert not (self.query.select and self.query.default_cols)\n209         if self.query.default_cols:\n210             cols = self.get_default_columns()\n211         else:\n212             # self.query.select is a special case. These columns never go to\n213             # any model.\n214             cols = self.query.select\n215         if cols:\n216             select_list = []\n217             for col in cols:\n218                 select_list.append(select_idx)\n219                 select.append((col, None))\n220                 select_idx += 1\n221             klass_info = {\n222                 'model': self.query.model,\n223                 'select_fields': select_list,\n224             }\n225         for alias, annotation in self.query.annotation_select.items():\n226             annotations[alias] = select_idx\n227             select.append((annotation, alias))\n228             select_idx += 1\n229 \n230         if self.query.select_related:\n231             related_klass_infos = self.get_related_selections(select)\n232             klass_info['related_klass_infos'] = related_klass_infos\n233 \n234             def get_select_from_parent(klass_info):\n235                 for ki in klass_info['related_klass_infos']:\n236                     if ki['from_parent']:\n237                         ki['select_fields'] = (klass_info['select_fields'] +\n238                                                ki['select_fields'])\n239                     get_select_from_parent(ki)\n240             get_select_from_parent(klass_info)\n241 \n242         ret = []\n243         for col, alias in select:\n244             try:\n245                 sql, params = self.compile(col, select_format=True)\n246             except EmptyResultSet:\n247                 # Select a predicate that's always False.\n248                 sql, params = '0', ()\n249             ret.append((col, (sql, params), alias))\n250         return ret, klass_info, annotations\n251 \n252     def get_order_by(self):\n253         \"\"\"\n254         Return a list of 2-tuples of form (expr, (sql, params, is_ref)) for the\n255         ORDER BY clause.\n256 \n257         The order_by clause can alter the select clause (for example it\n258         can add aliases to clauses that do not yet have one, or it can\n259         add totally new select clauses).\n260         \"\"\"\n261         if self.query.extra_order_by:\n262             ordering = self.query.extra_order_by\n263         elif not self.query.default_ordering:\n264             ordering = self.query.order_by\n265         elif self.query.order_by:\n266             ordering = self.query.order_by\n267         elif self.query.get_meta().ordering:\n268             ordering = self.query.get_meta().ordering\n269             self._meta_ordering = ordering\n270         else:\n271             ordering = []\n272         if self.query.standard_ordering:\n273             asc, desc = ORDER_DIR['ASC']\n274         else:\n275             asc, desc = ORDER_DIR['DESC']\n276 \n277         order_by = []\n278         for field in ordering:\n279             if hasattr(field, 'resolve_expression'):\n280                 if not isinstance(field, OrderBy):\n281                     field = field.asc()\n282                 if not self.query.standard_ordering:\n283                     field.reverse_ordering()\n284                 order_by.append((field, False))\n285                 continue\n286             if field == '?':  # random\n287                 order_by.append((OrderBy(Random()), False))\n288                 continue\n289 \n290             col, order = get_order_dir(field, asc)\n291             descending = order == 'DESC'\n292 \n293             if col in self.query.annotation_select:\n294                 # Reference to expression in SELECT clause\n295                 order_by.append((\n296                     OrderBy(Ref(col, self.query.annotation_select[col]), descending=descending),\n297                     True))\n298                 continue\n299             if col in self.query.annotations:\n300                 # References to an expression which is masked out of the SELECT clause\n301                 order_by.append((\n302                     OrderBy(self.query.annotations[col], descending=descending),\n303                     False))\n304                 continue\n305 \n306             if '.' in field:\n307                 # This came in through an extra(order_by=...) addition. Pass it\n308                 # on verbatim.\n309                 table, col = col.split('.', 1)\n310                 order_by.append((\n311                     OrderBy(\n312                         RawSQL('%s.%s' % (self.quote_name_unless_alias(table), col), []),\n313                         descending=descending\n314                     ), False))\n315                 continue\n316 \n317             if not self.query.extra or col not in self.query.extra:\n318                 # 'col' is of the form 'field' or 'field1__field2' or\n319                 # '-field1__field2__field', etc.\n320                 order_by.extend(self.find_ordering_name(\n321                     field, self.query.get_meta(), default_order=asc))\n322             else:\n323                 if col not in self.query.extra_select:\n324                     order_by.append((\n325                         OrderBy(RawSQL(*self.query.extra[col]), descending=descending),\n326                         False))\n327                 else:\n328                     order_by.append((\n329                         OrderBy(Ref(col, RawSQL(*self.query.extra[col])), descending=descending),\n330                         True))\n331         result = []\n332         seen = set()\n333 \n334         for expr, is_ref in order_by:\n335             resolved = expr.resolve_expression(self.query, allow_joins=True, reuse=None)\n336             if self.query.combinator:\n337                 src = resolved.get_source_expressions()[0]\n338                 # Relabel order by columns to raw numbers if this is a combined\n339                 # query; necessary since the columns can't be referenced by the\n340                 # fully qualified name and the simple column names may collide.\n341                 for idx, (sel_expr, _, col_alias) in enumerate(self.select):\n342                     if is_ref and col_alias == src.refs:\n343                         src = src.source\n344                     elif col_alias:\n345                         continue\n346                     if src == sel_expr:\n347                         resolved.set_source_expressions([RawSQL('%d' % (idx + 1), ())])\n348                         break\n349                 else:\n350                     raise DatabaseError('ORDER BY term does not match any column in the result set.')\n351             sql, params = self.compile(resolved)\n352             # Don't add the same column twice, but the order direction is\n353             # not taken into account so we strip it. When this entire method\n354             # is refactored into expressions, then we can check each part as we\n355             # generate it.\n356             without_ordering = self.ordering_parts.search(sql).group(1)\n357             params_hash = make_hashable(params)\n358             if (without_ordering, params_hash) in seen:\n359                 continue\n360             seen.add((without_ordering, params_hash))\n361             result.append((resolved, (sql, params, is_ref)))\n362         return result\n363 \n364     def get_extra_select(self, order_by, select):\n365         extra_select = []\n366         if self.query.distinct and not self.query.distinct_fields:\n367             select_sql = [t[1] for t in select]\n368             for expr, (sql, params, is_ref) in order_by:\n369                 without_ordering = self.ordering_parts.search(sql).group(1)\n370                 if not is_ref and (without_ordering, params) not in select_sql:\n371                     extra_select.append((expr, (without_ordering, params), None))\n372         return extra_select\n373 \n374     def quote_name_unless_alias(self, name):\n375         \"\"\"\n376         A wrapper around connection.ops.quote_name that doesn't quote aliases\n377         for table names. This avoids problems with some SQL dialects that treat\n378         quoted strings specially (e.g. PostgreSQL).\n379         \"\"\"\n380         if name in self.quote_cache:\n381             return self.quote_cache[name]\n382         if ((name in self.query.alias_map and name not in self.query.table_map) or\n383                 name in self.query.extra_select or (\n384                     name in self.query.external_aliases and name not in self.query.table_map)):\n385             self.quote_cache[name] = name\n386             return name\n387         r = self.connection.ops.quote_name(name)\n388         self.quote_cache[name] = r\n389         return r\n390 \n391     def compile(self, node, select_format=False):\n392         vendor_impl = getattr(node, 'as_' + self.connection.vendor, None)\n393         if vendor_impl:\n394             sql, params = vendor_impl(self, self.connection)\n395         else:\n396             sql, params = node.as_sql(self, self.connection)\n397         if select_format is FORCE or (select_format and not self.query.subquery):\n398             return node.output_field.select_format(self, sql, params)\n399         return sql, params\n400 \n401     def get_combinator_sql(self, combinator, all):\n402         features = self.connection.features\n403         compilers = [\n404             query.get_compiler(self.using, self.connection)\n405             for query in self.query.combined_queries if not query.is_empty()\n406         ]\n407         if not features.supports_slicing_ordering_in_compound:\n408             for query, compiler in zip(self.query.combined_queries, compilers):\n409                 if query.low_mark or query.high_mark:\n410                     raise DatabaseError('LIMIT/OFFSET not allowed in subqueries of compound statements.')\n411                 if compiler.get_order_by():\n412                     raise DatabaseError('ORDER BY not allowed in subqueries of compound statements.')\n413         parts = ()\n414         for compiler in compilers:\n415             try:\n416                 # If the columns list is limited, then all combined queries\n417                 # must have the same columns list. Set the selects defined on\n418                 # the query on all combined queries, if not already set.\n419                 if not compiler.query.values_select and self.query.values_select:\n420                     compiler.query.set_values((\n421                         *self.query.extra_select,\n422                         *self.query.values_select,\n423                         *self.query.annotation_select,\n424                     ))\n425                 part_sql, part_args = compiler.as_sql()\n426                 if compiler.query.combinator:\n427                     # Wrap in a subquery if wrapping in parentheses isn't\n428                     # supported.\n429                     if not features.supports_parentheses_in_compound:\n430                         part_sql = 'SELECT * FROM ({})'.format(part_sql)\n431                     # Add parentheses when combining with compound query if not\n432                     # already added for all compound queries.\n433                     elif not features.supports_slicing_ordering_in_compound:\n434                         part_sql = '({})'.format(part_sql)\n435                 parts += ((part_sql, part_args),)\n436             except EmptyResultSet:\n437                 # Omit the empty queryset with UNION and with DIFFERENCE if the\n438                 # first queryset is nonempty.\n439                 if combinator == 'union' or (combinator == 'difference' and parts):\n440                     continue\n441                 raise\n442         if not parts:\n443             raise EmptyResultSet\n444         combinator_sql = self.connection.ops.set_operators[combinator]\n445         if all and combinator == 'union':\n446             combinator_sql += ' ALL'\n447         braces = '({})' if features.supports_slicing_ordering_in_compound else '{}'\n448         sql_parts, args_parts = zip(*((braces.format(sql), args) for sql, args in parts))\n449         result = [' {} '.format(combinator_sql).join(sql_parts)]\n450         params = []\n451         for part in args_parts:\n452             params.extend(part)\n453         return result, params\n454 \n455     def as_sql(self, with_limits=True, with_col_aliases=False):\n456         \"\"\"\n457         Create the SQL for this query. Return the SQL string and list of\n458         parameters.\n459 \n460         If 'with_limits' is False, any limit/offset information is not included\n461         in the query.\n462         \"\"\"\n463         refcounts_before = self.query.alias_refcount.copy()\n464         try:\n465             extra_select, order_by, group_by = self.pre_sql_setup()\n466             for_update_part = None\n467             # Is a LIMIT/OFFSET clause needed?\n468             with_limit_offset = with_limits and (self.query.high_mark is not None or self.query.low_mark)\n469             combinator = self.query.combinator\n470             features = self.connection.features\n471             if combinator:\n472                 if not getattr(features, 'supports_select_{}'.format(combinator)):\n473                     raise NotSupportedError('{} is not supported on this database backend.'.format(combinator))\n474                 result, params = self.get_combinator_sql(combinator, self.query.combinator_all)\n475             else:\n476                 distinct_fields, distinct_params = self.get_distinct()\n477                 # This must come after 'select', 'ordering', and 'distinct'\n478                 # (see docstring of get_from_clause() for details).\n479                 from_, f_params = self.get_from_clause()\n480                 where, w_params = self.compile(self.where) if self.where is not None else (\"\", [])\n481                 having, h_params = self.compile(self.having) if self.having is not None else (\"\", [])\n482                 result = ['SELECT']\n483                 params = []\n484 \n485                 if self.query.distinct:\n486                     distinct_result, distinct_params = self.connection.ops.distinct_sql(\n487                         distinct_fields,\n488                         distinct_params,\n489                     )\n490                     result += distinct_result\n491                     params += distinct_params\n492 \n493                 out_cols = []\n494                 col_idx = 1\n495                 for _, (s_sql, s_params), alias in self.select + extra_select:\n496                     if alias:\n497                         s_sql = '%s AS %s' % (s_sql, self.connection.ops.quote_name(alias))\n498                     elif with_col_aliases:\n499                         s_sql = '%s AS %s' % (s_sql, 'Col%d' % col_idx)\n500                         col_idx += 1\n501                     params.extend(s_params)\n502                     out_cols.append(s_sql)\n503 \n504                 result += [', '.join(out_cols), 'FROM', *from_]\n505                 params.extend(f_params)\n506 \n507                 if self.query.select_for_update and self.connection.features.has_select_for_update:\n508                     if self.connection.get_autocommit():\n509                         raise TransactionManagementError('select_for_update cannot be used outside of a transaction.')\n510 \n511                     if with_limit_offset and not self.connection.features.supports_select_for_update_with_limit:\n512                         raise NotSupportedError(\n513                             'LIMIT/OFFSET is not supported with '\n514                             'select_for_update on this database backend.'\n515                         )\n516                     nowait = self.query.select_for_update_nowait\n517                     skip_locked = self.query.select_for_update_skip_locked\n518                     of = self.query.select_for_update_of\n519                     # If it's a NOWAIT/SKIP LOCKED/OF query but the backend\n520                     # doesn't support it, raise NotSupportedError to prevent a\n521                     # possible deadlock.\n522                     if nowait and not self.connection.features.has_select_for_update_nowait:\n523                         raise NotSupportedError('NOWAIT is not supported on this database backend.')\n524                     elif skip_locked and not self.connection.features.has_select_for_update_skip_locked:\n525                         raise NotSupportedError('SKIP LOCKED is not supported on this database backend.')\n526                     elif of and not self.connection.features.has_select_for_update_of:\n527                         raise NotSupportedError('FOR UPDATE OF is not supported on this database backend.')\n528                     for_update_part = self.connection.ops.for_update_sql(\n529                         nowait=nowait,\n530                         skip_locked=skip_locked,\n531                         of=self.get_select_for_update_of_arguments(),\n532                     )\n533 \n534                 if for_update_part and self.connection.features.for_update_after_from:\n535                     result.append(for_update_part)\n536 \n537                 if where:\n538                     result.append('WHERE %s' % where)\n539                     params.extend(w_params)\n540 \n541                 grouping = []\n542                 for g_sql, g_params in group_by:\n543                     grouping.append(g_sql)\n544                     params.extend(g_params)\n545                 if grouping:\n546                     if distinct_fields:\n547                         raise NotImplementedError('annotate() + distinct(fields) is not implemented.')\n548                     order_by = order_by or self.connection.ops.force_no_ordering()\n549                     result.append('GROUP BY %s' % ', '.join(grouping))\n550                     if self._meta_ordering:\n551                         # When the deprecation ends, replace with:\n552                         # order_by = None\n553                         warnings.warn(\n554                             \"%s QuerySet won't use Meta.ordering in Django 3.1. \"\n555                             \"Add .order_by('%s') to retain the current query.\" % (\n556                                 self.query.model.__name__,\n557                                 \"', '\".join(self._meta_ordering)\n558                             ),\n559                             RemovedInDjango31Warning,\n560                             stacklevel=4,\n561                         )\n562                 if having:\n563                     result.append('HAVING %s' % having)\n564                     params.extend(h_params)\n565 \n566             if self.query.explain_query:\n567                 result.insert(0, self.connection.ops.explain_query_prefix(\n568                     self.query.explain_format,\n569                     **self.query.explain_options\n570                 ))\n571 \n572             if order_by:\n573                 ordering = []\n574                 for _, (o_sql, o_params, _) in order_by:\n575                     ordering.append(o_sql)\n576                     params.extend(o_params)\n577                 result.append('ORDER BY %s' % ', '.join(ordering))\n578 \n579             if with_limit_offset:\n580                 result.append(self.connection.ops.limit_offset_sql(self.query.low_mark, self.query.high_mark))\n581 \n582             if for_update_part and not self.connection.features.for_update_after_from:\n583                 result.append(for_update_part)\n584 \n585             if self.query.subquery and extra_select:\n586                 # If the query is used as a subquery, the extra selects would\n587                 # result in more columns than the left-hand side expression is\n588                 # expecting. This can happen when a subquery uses a combination\n589                 # of order_by() and distinct(), forcing the ordering expressions\n590                 # to be selected as well. Wrap the query in another subquery\n591                 # to exclude extraneous selects.\n592                 sub_selects = []\n593                 sub_params = []\n594                 for index, (select, _, alias) in enumerate(self.select, start=1):\n595                     if not alias and with_col_aliases:\n596                         alias = 'col%d' % index\n597                     if alias:\n598                         sub_selects.append(\"%s.%s\" % (\n599                             self.connection.ops.quote_name('subquery'),\n600                             self.connection.ops.quote_name(alias),\n601                         ))\n602                     else:\n603                         select_clone = select.relabeled_clone({select.alias: 'subquery'})\n604                         subselect, subparams = select_clone.as_sql(self, self.connection)\n605                         sub_selects.append(subselect)\n606                         sub_params.extend(subparams)\n607                 return 'SELECT %s FROM (%s) subquery' % (\n608                     ', '.join(sub_selects),\n609                     ' '.join(result),\n610                 ), tuple(sub_params + params)\n611 \n612             return ' '.join(result), tuple(params)\n613         finally:\n614             # Finally do cleanup - get rid of the joins we created above.\n615             self.query.reset_refcounts(refcounts_before)\n616 \n617     def get_default_columns(self, start_alias=None, opts=None, from_parent=None):\n618         \"\"\"\n619         Compute the default columns for selecting every field in the base\n620         model. Will sometimes be called to pull in related models (e.g. via\n621         select_related), in which case \"opts\" and \"start_alias\" will be given\n622         to provide a starting point for the traversal.\n623 \n624         Return a list of strings, quoted appropriately for use in SQL\n625         directly, as well as a set of aliases used in the select statement (if\n626         'as_pairs' is True, return a list of (alias, col_name) pairs instead\n627         of strings as the first component and None as the second component).\n628         \"\"\"\n629         result = []\n630         if opts is None:\n631             opts = self.query.get_meta()\n632         only_load = self.deferred_to_columns()\n633         start_alias = start_alias or self.query.get_initial_alias()\n634         # The 'seen_models' is used to optimize checking the needed parent\n635         # alias for a given field. This also includes None -> start_alias to\n636         # be used by local fields.\n637         seen_models = {None: start_alias}\n638 \n639         for field in opts.concrete_fields:\n640             model = field.model._meta.concrete_model\n641             # A proxy model will have a different model and concrete_model. We\n642             # will assign None if the field belongs to this model.\n643             if model == opts.model:\n644                 model = None\n645             if from_parent and model is not None and issubclass(\n646                     from_parent._meta.concrete_model, model._meta.concrete_model):\n647                 # Avoid loading data for already loaded parents.\n648                 # We end up here in the case select_related() resolution\n649                 # proceeds from parent model to child model. In that case the\n650                 # parent model data is already present in the SELECT clause,\n651                 # and we want to avoid reloading the same data again.\n652                 continue\n653             if field.model in only_load and field.attname not in only_load[field.model]:\n654                 continue\n655             alias = self.query.join_parent_model(opts, model, start_alias,\n656                                                  seen_models)\n657             column = field.get_col(alias)\n658             result.append(column)\n659         return result\n660 \n661     def get_distinct(self):\n662         \"\"\"\n663         Return a quoted list of fields to use in DISTINCT ON part of the query.\n664 \n665         This method can alter the tables in the query, and thus it must be\n666         called before get_from_clause().\n667         \"\"\"\n668         result = []\n669         params = []\n670         opts = self.query.get_meta()\n671 \n672         for name in self.query.distinct_fields:\n673             parts = name.split(LOOKUP_SEP)\n674             _, targets, alias, joins, path, _, transform_function = self._setup_joins(parts, opts, None)\n675             targets, alias, _ = self.query.trim_joins(targets, joins, path)\n676             for target in targets:\n677                 if name in self.query.annotation_select:\n678                     result.append(name)\n679                 else:\n680                     r, p = self.compile(transform_function(target, alias))\n681                     result.append(r)\n682                     params.append(p)\n683         return result, params\n684 \n685     def find_ordering_name(self, name, opts, alias=None, default_order='ASC',\n686                            already_seen=None):\n687         \"\"\"\n688         Return the table alias (the name might be ambiguous, the alias will\n689         not be) and column name for ordering by the given 'name' parameter.\n690         The 'name' is of the form 'field1__field2__...__fieldN'.\n691         \"\"\"\n692         name, order = get_order_dir(name, default_order)\n693         descending = order == 'DESC'\n694         pieces = name.split(LOOKUP_SEP)\n695         field, targets, alias, joins, path, opts, transform_function = self._setup_joins(pieces, opts, alias)\n696 \n697         # If we get to this point and the field is a relation to another model,\n698         # append the default ordering for that model unless the attribute name\n699         # of the field is specified.\n700         if field.is_relation and opts.ordering and getattr(field, 'attname', None) != name:\n701             # Firstly, avoid infinite loops.\n702             already_seen = already_seen or set()\n703             join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)\n704             if join_tuple in already_seen:\n705                 raise FieldError('Infinite loop caused by ordering.')\n706             already_seen.add(join_tuple)\n707 \n708             results = []\n709             for item in opts.ordering:\n710                 results.extend(self.find_ordering_name(item, opts, alias,\n711                                                        order, already_seen))\n712             return results\n713         targets, alias, _ = self.query.trim_joins(targets, joins, path)\n714         return [(OrderBy(transform_function(t, alias), descending=descending), False) for t in targets]\n715 \n716     def _setup_joins(self, pieces, opts, alias):\n717         \"\"\"\n718         Helper method for get_order_by() and get_distinct().\n719 \n720         get_ordering() and get_distinct() must produce same target columns on\n721         same input, as the prefixes of get_ordering() and get_distinct() must\n722         match. Executing SQL where this is not true is an error.\n723         \"\"\"\n724         alias = alias or self.query.get_initial_alias()\n725         field, targets, opts, joins, path, transform_function = self.query.setup_joins(pieces, opts, alias)\n726         alias = joins[-1]\n727         return field, targets, alias, joins, path, opts, transform_function\n728 \n729     def get_from_clause(self):\n730         \"\"\"\n731         Return a list of strings that are joined together to go after the\n732         \"FROM\" part of the query, as well as a list any extra parameters that\n733         need to be included. Subclasses, can override this to create a\n734         from-clause via a \"select\".\n735 \n736         This should only be called after any SQL construction methods that\n737         might change the tables that are needed. This means the select columns,\n738         ordering, and distinct must be done first.\n739         \"\"\"\n740         result = []\n741         params = []\n742         for alias in tuple(self.query.alias_map):\n743             if not self.query.alias_refcount[alias]:\n744                 continue\n745             try:\n746                 from_clause = self.query.alias_map[alias]\n747             except KeyError:\n748                 # Extra tables can end up in self.tables, but not in the\n749                 # alias_map if they aren't in a join. That's OK. We skip them.\n750                 continue\n751             clause_sql, clause_params = self.compile(from_clause)\n752             result.append(clause_sql)\n753             params.extend(clause_params)\n754         for t in self.query.extra_tables:\n755             alias, _ = self.query.table_alias(t)\n756             # Only add the alias if it's not already present (the table_alias()\n757             # call increments the refcount, so an alias refcount of one means\n758             # this is the only reference).\n759             if alias not in self.query.alias_map or self.query.alias_refcount[alias] == 1:\n760                 result.append(', %s' % self.quote_name_unless_alias(alias))\n761         return result, params\n762 \n763     def get_related_selections(self, select, opts=None, root_alias=None, cur_depth=1,\n764                                requested=None, restricted=None):\n765         \"\"\"\n766         Fill in the information needed for a select_related query. The current\n767         depth is measured as the number of connections away from the root model\n768         (for example, cur_depth=1 means we are looking at models with direct\n769         connections to the root model).\n770         \"\"\"\n771         def _get_field_choices():\n772             direct_choices = (f.name for f in opts.fields if f.is_relation)\n773             reverse_choices = (\n774                 f.field.related_query_name()\n775                 for f in opts.related_objects if f.field.unique\n776             )\n777             return chain(direct_choices, reverse_choices, self.query._filtered_relations)\n778 \n779         related_klass_infos = []\n780         if not restricted and cur_depth > self.query.max_depth:\n781             # We've recursed far enough; bail out.\n782             return related_klass_infos\n783 \n784         if not opts:\n785             opts = self.query.get_meta()\n786             root_alias = self.query.get_initial_alias()\n787         only_load = self.query.get_loaded_field_names()\n788 \n789         # Setup for the case when only particular related fields should be\n790         # included in the related selection.\n791         fields_found = set()\n792         if requested is None:\n793             restricted = isinstance(self.query.select_related, dict)\n794             if restricted:\n795                 requested = self.query.select_related\n796 \n797         def get_related_klass_infos(klass_info, related_klass_infos):\n798             klass_info['related_klass_infos'] = related_klass_infos\n799 \n800         for f in opts.fields:\n801             field_model = f.model._meta.concrete_model\n802             fields_found.add(f.name)\n803 \n804             if restricted:\n805                 next = requested.get(f.name, {})\n806                 if not f.is_relation:\n807                     # If a non-related field is used like a relation,\n808                     # or if a single non-relational field is given.\n809                     if next or f.name in requested:\n810                         raise FieldError(\n811                             \"Non-relational field given in select_related: '%s'. \"\n812                             \"Choices are: %s\" % (\n813                                 f.name,\n814                                 \", \".join(_get_field_choices()) or '(none)',\n815                             )\n816                         )\n817             else:\n818                 next = False\n819 \n820             if not select_related_descend(f, restricted, requested,\n821                                           only_load.get(field_model)):\n822                 continue\n823             klass_info = {\n824                 'model': f.remote_field.model,\n825                 'field': f,\n826                 'reverse': False,\n827                 'local_setter': f.set_cached_value,\n828                 'remote_setter': f.remote_field.set_cached_value if f.unique else lambda x, y: None,\n829                 'from_parent': False,\n830             }\n831             related_klass_infos.append(klass_info)\n832             select_fields = []\n833             _, _, _, joins, _, _ = self.query.setup_joins(\n834                 [f.name], opts, root_alias)\n835             alias = joins[-1]\n836             columns = self.get_default_columns(start_alias=alias, opts=f.remote_field.model._meta)\n837             for col in columns:\n838                 select_fields.append(len(select))\n839                 select.append((col, None))\n840             klass_info['select_fields'] = select_fields\n841             next_klass_infos = self.get_related_selections(\n842                 select, f.remote_field.model._meta, alias, cur_depth + 1, next, restricted)\n843             get_related_klass_infos(klass_info, next_klass_infos)\n844 \n845         if restricted:\n846             related_fields = [\n847                 (o.field, o.related_model)\n848                 for o in opts.related_objects\n849                 if o.field.unique and not o.many_to_many\n850             ]\n851             for f, model in related_fields:\n852                 if not select_related_descend(f, restricted, requested,\n853                                               only_load.get(model), reverse=True):\n854                     continue\n855 \n856                 related_field_name = f.related_query_name()\n857                 fields_found.add(related_field_name)\n858 \n859                 join_info = self.query.setup_joins([related_field_name], opts, root_alias)\n860                 alias = join_info.joins[-1]\n861                 from_parent = issubclass(model, opts.model) and model is not opts.model\n862                 klass_info = {\n863                     'model': model,\n864                     'field': f,\n865                     'reverse': True,\n866                     'local_setter': f.remote_field.set_cached_value,\n867                     'remote_setter': f.set_cached_value,\n868                     'from_parent': from_parent,\n869                 }\n870                 related_klass_infos.append(klass_info)\n871                 select_fields = []\n872                 columns = self.get_default_columns(\n873                     start_alias=alias, opts=model._meta, from_parent=opts.model)\n874                 for col in columns:\n875                     select_fields.append(len(select))\n876                     select.append((col, None))\n877                 klass_info['select_fields'] = select_fields\n878                 next = requested.get(f.related_query_name(), {})\n879                 next_klass_infos = self.get_related_selections(\n880                     select, model._meta, alias, cur_depth + 1,\n881                     next, restricted)\n882                 get_related_klass_infos(klass_info, next_klass_infos)\n883             for name in list(requested):\n884                 # Filtered relations work only on the topmost level.\n885                 if cur_depth > 1:\n886                     break\n887                 if name in self.query._filtered_relations:\n888                     fields_found.add(name)\n889                     f, _, join_opts, joins, _, _ = self.query.setup_joins([name], opts, root_alias)\n890                     model = join_opts.model\n891                     alias = joins[-1]\n892                     from_parent = issubclass(model, opts.model) and model is not opts.model\n893 \n894                     def local_setter(obj, from_obj):\n895                         # Set a reverse fk object when relation is non-empty.\n896                         if from_obj:\n897                             f.remote_field.set_cached_value(from_obj, obj)\n898 \n899                     def remote_setter(obj, from_obj):\n900                         setattr(from_obj, name, obj)\n901                     klass_info = {\n902                         'model': model,\n903                         'field': f,\n904                         'reverse': True,\n905                         'local_setter': local_setter,\n906                         'remote_setter': remote_setter,\n907                         'from_parent': from_parent,\n908                     }\n909                     related_klass_infos.append(klass_info)\n910                     select_fields = []\n911                     columns = self.get_default_columns(\n912                         start_alias=alias, opts=model._meta,\n913                         from_parent=opts.model,\n914                     )\n915                     for col in columns:\n916                         select_fields.append(len(select))\n917                         select.append((col, None))\n918                     klass_info['select_fields'] = select_fields\n919                     next_requested = requested.get(name, {})\n920                     next_klass_infos = self.get_related_selections(\n921                         select, opts=model._meta, root_alias=alias,\n922                         cur_depth=cur_depth + 1, requested=next_requested,\n923                         restricted=restricted,\n924                     )\n925                     get_related_klass_infos(klass_info, next_klass_infos)\n926             fields_not_found = set(requested).difference(fields_found)\n927             if fields_not_found:\n928                 invalid_fields = (\"'%s'\" % s for s in fields_not_found)\n929                 raise FieldError(\n930                     'Invalid field name(s) given in select_related: %s. '\n931                     'Choices are: %s' % (\n932                         ', '.join(invalid_fields),\n933                         ', '.join(_get_field_choices()) or '(none)',\n934                     )\n935                 )\n936         return related_klass_infos\n937 \n938     def get_select_for_update_of_arguments(self):\n939         \"\"\"\n940         Return a quoted list of arguments for the SELECT FOR UPDATE OF part of\n941         the query.\n942         \"\"\"\n943         def _get_field_choices():\n944             \"\"\"Yield all allowed field paths in breadth-first search order.\"\"\"\n945             queue = collections.deque([(None, self.klass_info)])\n946             while queue:\n947                 parent_path, klass_info = queue.popleft()\n948                 if parent_path is None:\n949                     path = []\n950                     yield 'self'\n951                 else:\n952                     field = klass_info['field']\n953                     if klass_info['reverse']:\n954                         field = field.remote_field\n955                     path = parent_path + [field.name]\n956                     yield LOOKUP_SEP.join(path)\n957                 queue.extend(\n958                     (path, klass_info)\n959                     for klass_info in klass_info.get('related_klass_infos', [])\n960                 )\n961         result = []\n962         invalid_names = []\n963         for name in self.query.select_for_update_of:\n964             parts = [] if name == 'self' else name.split(LOOKUP_SEP)\n965             klass_info = self.klass_info\n966             for part in parts:\n967                 for related_klass_info in klass_info.get('related_klass_infos', []):\n968                     field = related_klass_info['field']\n969                     if related_klass_info['reverse']:\n970                         field = field.remote_field\n971                     if field.name == part:\n972                         klass_info = related_klass_info\n973                         break\n974                 else:\n975                     klass_info = None\n976                     break\n977             if klass_info is None:\n978                 invalid_names.append(name)\n979                 continue\n980             select_index = klass_info['select_fields'][0]\n981             col = self.select[select_index][0]\n982             if self.connection.features.select_for_update_of_column:\n983                 result.append(self.compile(col)[0])\n984             else:\n985                 result.append(self.quote_name_unless_alias(col.alias))\n986         if invalid_names:\n987             raise FieldError(\n988                 'Invalid field name(s) given in select_for_update(of=(...)): %s. '\n989                 'Only relational fields followed in the query are allowed. '\n990                 'Choices are: %s.' % (\n991                     ', '.join(invalid_names),\n992                     ', '.join(_get_field_choices()),\n993                 )\n994             )\n995         return result\n996 \n997     def deferred_to_columns(self):\n998         \"\"\"\n999         Convert the self.deferred_loading data structure to mapping of table\n1000         names to sets of column names which are to be loaded. Return the\n1001         dictionary.\n1002         \"\"\"\n1003         columns = {}\n1004         self.query.deferred_to_data(columns, self.query.get_loaded_field_names_cb)\n1005         return columns\n1006 \n1007     def get_converters(self, expressions):\n1008         converters = {}\n1009         for i, expression in enumerate(expressions):\n1010             if expression:\n1011                 backend_converters = self.connection.ops.get_db_converters(expression)\n1012                 field_converters = expression.get_db_converters(self.connection)\n1013                 if backend_converters or field_converters:\n1014                     converters[i] = (backend_converters + field_converters, expression)\n1015         return converters\n1016 \n1017     def apply_converters(self, rows, converters):\n1018         connection = self.connection\n1019         converters = list(converters.items())\n1020         for row in map(list, rows):\n1021             for pos, (convs, expression) in converters:\n1022                 value = row[pos]\n1023                 for converter in convs:\n1024                     value = converter(value, expression, connection)\n1025                 row[pos] = value\n1026             yield row\n1027 \n1028     def results_iter(self, results=None, tuple_expected=False, chunked_fetch=False,\n1029                      chunk_size=GET_ITERATOR_CHUNK_SIZE):\n1030         \"\"\"Return an iterator over the results from executing this query.\"\"\"\n1031         if results is None:\n1032             results = self.execute_sql(MULTI, chunked_fetch=chunked_fetch, chunk_size=chunk_size)\n1033         fields = [s[0] for s in self.select[0:self.col_count]]\n1034         converters = self.get_converters(fields)\n1035         rows = chain.from_iterable(results)\n1036         if converters:\n1037             rows = self.apply_converters(rows, converters)\n1038             if tuple_expected:\n1039                 rows = map(tuple, rows)\n1040         return rows\n1041 \n1042     def has_results(self):\n1043         \"\"\"\n1044         Backends (e.g. NoSQL) can override this in order to use optimized\n1045         versions of \"query has any results.\"\n1046         \"\"\"\n1047         # This is always executed on a query clone, so we can modify self.query\n1048         self.query.add_extra({'a': 1}, None, None, None, None, None)\n1049         self.query.set_extra_mask(['a'])\n1050         return bool(self.execute_sql(SINGLE))\n1051 \n1052     def execute_sql(self, result_type=MULTI, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):\n1053         \"\"\"\n1054         Run the query against the database and return the result(s). The\n1055         return value is a single data item if result_type is SINGLE, or an\n1056         iterator over the results if the result_type is MULTI.\n1057 \n1058         result_type is either MULTI (use fetchmany() to retrieve all rows),\n1059         SINGLE (only retrieve a single row), or None. In this last case, the\n1060         cursor is returned if any query is executed, since it's used by\n1061         subclasses such as InsertQuery). It's possible, however, that no query\n1062         is needed, as the filters describe an empty set. In that case, None is\n1063         returned, to avoid any unnecessary database interaction.\n1064         \"\"\"\n1065         result_type = result_type or NO_RESULTS\n1066         try:\n1067             sql, params = self.as_sql()\n1068             if not sql:\n1069                 raise EmptyResultSet\n1070         except EmptyResultSet:\n1071             if result_type == MULTI:\n1072                 return iter([])\n1073             else:\n1074                 return\n1075         if chunked_fetch:\n1076             cursor = self.connection.chunked_cursor()\n1077         else:\n1078             cursor = self.connection.cursor()\n1079         try:\n1080             cursor.execute(sql, params)\n1081         except Exception:\n1082             # Might fail for server-side cursors (e.g. connection closed)\n1083             cursor.close()\n1084             raise\n1085 \n1086         if result_type == CURSOR:\n1087             # Give the caller the cursor to process and close.\n1088             return cursor\n1089         if result_type == SINGLE:\n1090             try:\n1091                 val = cursor.fetchone()\n1092                 if val:\n1093                     return val[0:self.col_count]\n1094                 return val\n1095             finally:\n1096                 # done with the cursor\n1097                 cursor.close()\n1098         if result_type == NO_RESULTS:\n1099             cursor.close()\n1100             return\n1101 \n1102         result = cursor_iter(\n1103             cursor, self.connection.features.empty_fetchmany_value,\n1104             self.col_count if self.has_extra_select else None,\n1105             chunk_size,\n1106         )\n1107         if not chunked_fetch or not self.connection.features.can_use_chunked_reads:\n1108             try:\n1109                 # If we are using non-chunked reads, we return the same data\n1110                 # structure as normally, but ensure it is all read into memory\n1111                 # before going any further. Use chunked_fetch if requested,\n1112                 # unless the database doesn't support it.\n1113                 return list(result)\n1114             finally:\n1115                 # done with the cursor\n1116                 cursor.close()\n1117         return result\n1118 \n1119     def as_subquery_condition(self, alias, columns, compiler):\n1120         qn = compiler.quote_name_unless_alias\n1121         qn2 = self.connection.ops.quote_name\n1122 \n1123         for index, select_col in enumerate(self.query.select):\n1124             lhs_sql, lhs_params = self.compile(select_col)\n1125             rhs = '%s.%s' % (qn(alias), qn2(columns[index]))\n1126             self.query.where.add(\n1127                 QueryWrapper('%s = %s' % (lhs_sql, rhs), lhs_params), 'AND')\n1128 \n1129         sql, params = self.as_sql()\n1130         return 'EXISTS (%s)' % sql, params\n1131 \n1132     def explain_query(self):\n1133         result = list(self.execute_sql())\n1134         # Some backends return 1 item tuples with strings, and others return\n1135         # tuples with integers and strings. Flatten them out into strings.\n1136         for row in result[0]:\n1137             if not isinstance(row, str):\n1138                 yield ' '.join(str(c) for c in row)\n1139             else:\n1140                 yield row\n1141 \n1142 \n1143 class SQLInsertCompiler(SQLCompiler):\n1144     return_id = False\n1145 \n1146     def field_as_sql(self, field, val):\n1147         \"\"\"\n1148         Take a field and a value intended to be saved on that field, and\n1149         return placeholder SQL and accompanying params. Check for raw values,\n1150         expressions, and fields with get_placeholder() defined in that order.\n1151 \n1152         When field is None, consider the value raw and use it as the\n1153         placeholder, with no corresponding parameters returned.\n1154         \"\"\"\n1155         if field is None:\n1156             # A field value of None means the value is raw.\n1157             sql, params = val, []\n1158         elif hasattr(val, 'as_sql'):\n1159             # This is an expression, let's compile it.\n1160             sql, params = self.compile(val)\n1161         elif hasattr(field, 'get_placeholder'):\n1162             # Some fields (e.g. geo fields) need special munging before\n1163             # they can be inserted.\n1164             sql, params = field.get_placeholder(val, self, self.connection), [val]\n1165         else:\n1166             # Return the common case for the placeholder\n1167             sql, params = '%s', [val]\n1168 \n1169         # The following hook is only used by Oracle Spatial, which sometimes\n1170         # needs to yield 'NULL' and [] as its placeholder and params instead\n1171         # of '%s' and [None]. The 'NULL' placeholder is produced earlier by\n1172         # OracleOperations.get_geom_placeholder(). The following line removes\n1173         # the corresponding None parameter. See ticket #10888.\n1174         params = self.connection.ops.modify_insert_params(sql, params)\n1175 \n1176         return sql, params\n1177 \n1178     def prepare_value(self, field, value):\n1179         \"\"\"\n1180         Prepare a value to be used in a query by resolving it if it is an\n1181         expression and otherwise calling the field's get_db_prep_save().\n1182         \"\"\"\n1183         if hasattr(value, 'resolve_expression'):\n1184             value = value.resolve_expression(self.query, allow_joins=False, for_save=True)\n1185             # Don't allow values containing Col expressions. They refer to\n1186             # existing columns on a row, but in the case of insert the row\n1187             # doesn't exist yet.\n1188             if value.contains_column_references:\n1189                 raise ValueError(\n1190                     'Failed to insert expression \"%s\" on %s. F() expressions '\n1191                     'can only be used to update, not to insert.' % (value, field)\n1192                 )\n1193             if value.contains_aggregate:\n1194                 raise FieldError(\n1195                     'Aggregate functions are not allowed in this query '\n1196                     '(%s=%r).' % (field.name, value)\n1197                 )\n1198             if value.contains_over_clause:\n1199                 raise FieldError(\n1200                     'Window expressions are not allowed in this query (%s=%r).'\n1201                     % (field.name, value)\n1202                 )\n1203         else:\n1204             value = field.get_db_prep_save(value, connection=self.connection)\n1205         return value\n1206 \n1207     def pre_save_val(self, field, obj):\n1208         \"\"\"\n1209         Get the given field's value off the given obj. pre_save() is used for\n1210         things like auto_now on DateTimeField. Skip it if this is a raw query.\n1211         \"\"\"\n1212         if self.query.raw:\n1213             return getattr(obj, field.attname)\n1214         return field.pre_save(obj, add=True)\n1215 \n1216     def assemble_as_sql(self, fields, value_rows):\n1217         \"\"\"\n1218         Take a sequence of N fields and a sequence of M rows of values, and\n1219         generate placeholder SQL and parameters for each field and value.\n1220         Return a pair containing:\n1221          * a sequence of M rows of N SQL placeholder strings, and\n1222          * a sequence of M rows of corresponding parameter values.\n1223 \n1224         Each placeholder string may contain any number of '%s' interpolation\n1225         strings, and each parameter row will contain exactly as many params\n1226         as the total number of '%s's in the corresponding placeholder row.\n1227         \"\"\"\n1228         if not value_rows:\n1229             return [], []\n1230 \n1231         # list of (sql, [params]) tuples for each object to be saved\n1232         # Shape: [n_objs][n_fields][2]\n1233         rows_of_fields_as_sql = (\n1234             (self.field_as_sql(field, v) for field, v in zip(fields, row))\n1235             for row in value_rows\n1236         )\n1237 \n1238         # tuple like ([sqls], [[params]s]) for each object to be saved\n1239         # Shape: [n_objs][2][n_fields]\n1240         sql_and_param_pair_rows = (zip(*row) for row in rows_of_fields_as_sql)\n1241 \n1242         # Extract separate lists for placeholders and params.\n1243         # Each of these has shape [n_objs][n_fields]\n1244         placeholder_rows, param_rows = zip(*sql_and_param_pair_rows)\n1245 \n1246         # Params for each field are still lists, and need to be flattened.\n1247         param_rows = [[p for ps in row for p in ps] for row in param_rows]\n1248 \n1249         return placeholder_rows, param_rows\n1250 \n1251     def as_sql(self):\n1252         # We don't need quote_name_unless_alias() here, since these are all\n1253         # going to be column names (so we can avoid the extra overhead).\n1254         qn = self.connection.ops.quote_name\n1255         opts = self.query.get_meta()\n1256         insert_statement = self.connection.ops.insert_statement(ignore_conflicts=self.query.ignore_conflicts)\n1257         result = ['%s %s' % (insert_statement, qn(opts.db_table))]\n1258         fields = self.query.fields or [opts.pk]\n1259         result.append('(%s)' % ', '.join(qn(f.column) for f in fields))\n1260 \n1261         if self.query.fields:\n1262             value_rows = [\n1263                 [self.prepare_value(field, self.pre_save_val(field, obj)) for field in fields]\n1264                 for obj in self.query.objs\n1265             ]\n1266         else:\n1267             # An empty object.\n1268             value_rows = [[self.connection.ops.pk_default_value()] for _ in self.query.objs]\n1269             fields = [None]\n1270 \n1271         # Currently the backends just accept values when generating bulk\n1272         # queries and generate their own placeholders. Doing that isn't\n1273         # necessary and it should be possible to use placeholders and\n1274         # expressions in bulk inserts too.\n1275         can_bulk = (not self.return_id and self.connection.features.has_bulk_insert)\n1276 \n1277         placeholder_rows, param_rows = self.assemble_as_sql(fields, value_rows)\n1278 \n1279         ignore_conflicts_suffix_sql = self.connection.ops.ignore_conflicts_suffix_sql(\n1280             ignore_conflicts=self.query.ignore_conflicts\n1281         )\n1282         if self.return_id and self.connection.features.can_return_columns_from_insert:\n1283             if self.connection.features.can_return_rows_from_bulk_insert:\n1284                 result.append(self.connection.ops.bulk_insert_sql(fields, placeholder_rows))\n1285                 params = param_rows\n1286             else:\n1287                 result.append(\"VALUES (%s)\" % \", \".join(placeholder_rows[0]))\n1288                 params = [param_rows[0]]\n1289             if ignore_conflicts_suffix_sql:\n1290                 result.append(ignore_conflicts_suffix_sql)\n1291             col = \"%s.%s\" % (qn(opts.db_table), qn(opts.pk.column))\n1292             r_fmt, r_params = self.connection.ops.return_insert_id()\n1293             # Skip empty r_fmt to allow subclasses to customize behavior for\n1294             # 3rd party backends. Refs #19096.\n1295             if r_fmt:\n1296                 result.append(r_fmt % col)\n1297                 params += [r_params]\n1298             return [(\" \".join(result), tuple(chain.from_iterable(params)))]\n1299 \n1300         if can_bulk:\n1301             result.append(self.connection.ops.bulk_insert_sql(fields, placeholder_rows))\n1302             if ignore_conflicts_suffix_sql:\n1303                 result.append(ignore_conflicts_suffix_sql)\n1304             return [(\" \".join(result), tuple(p for ps in param_rows for p in ps))]\n1305         else:\n1306             if ignore_conflicts_suffix_sql:\n1307                 result.append(ignore_conflicts_suffix_sql)\n1308             return [\n1309                 (\" \".join(result + [\"VALUES (%s)\" % \", \".join(p)]), vals)\n1310                 for p, vals in zip(placeholder_rows, param_rows)\n1311             ]\n1312 \n1313     def execute_sql(self, return_id=False):\n1314         assert not (\n1315             return_id and len(self.query.objs) != 1 and\n1316             not self.connection.features.can_return_rows_from_bulk_insert\n1317         )\n1318         self.return_id = return_id\n1319         with self.connection.cursor() as cursor:\n1320             for sql, params in self.as_sql():\n1321                 cursor.execute(sql, params)\n1322             if not return_id:\n1323                 return\n1324             if self.connection.features.can_return_rows_from_bulk_insert and len(self.query.objs) > 1:\n1325                 return self.connection.ops.fetch_returned_insert_ids(cursor)\n1326             if self.connection.features.can_return_columns_from_insert:\n1327                 assert len(self.query.objs) == 1\n1328                 return self.connection.ops.fetch_returned_insert_id(cursor)\n1329             return self.connection.ops.last_insert_id(\n1330                 cursor, self.query.get_meta().db_table, self.query.get_meta().pk.column\n1331             )\n1332 \n1333 \n1334 class SQLDeleteCompiler(SQLCompiler):\n1335     def as_sql(self):\n1336         \"\"\"\n1337         Create the SQL for this query. Return the SQL string and list of\n1338         parameters.\n1339         \"\"\"\n1340         assert len([t for t in self.query.alias_map if self.query.alias_refcount[t] > 0]) == 1, \\\n1341             \"Can only delete from one table at a time.\"\n1342         qn = self.quote_name_unless_alias\n1343         result = ['DELETE FROM %s' % qn(self.query.base_table)]\n1344         where, params = self.compile(self.query.where)\n1345         if where:\n1346             result.append('WHERE %s' % where)\n1347         return ' '.join(result), tuple(params)\n1348 \n1349 \n1350 class SQLUpdateCompiler(SQLCompiler):\n1351     def as_sql(self):\n1352         \"\"\"\n1353         Create the SQL for this query. Return the SQL string and list of\n1354         parameters.\n1355         \"\"\"\n1356         self.pre_sql_setup()\n1357         if not self.query.values:\n1358             return '', ()\n1359         qn = self.quote_name_unless_alias\n1360         values, update_params = [], []\n1361         for field, model, val in self.query.values:\n1362             if hasattr(val, 'resolve_expression'):\n1363                 val = val.resolve_expression(self.query, allow_joins=False, for_save=True)\n1364                 if val.contains_aggregate:\n1365                     raise FieldError(\n1366                         'Aggregate functions are not allowed in this query '\n1367                         '(%s=%r).' % (field.name, val)\n1368                     )\n1369                 if val.contains_over_clause:\n1370                     raise FieldError(\n1371                         'Window expressions are not allowed in this query '\n1372                         '(%s=%r).' % (field.name, val)\n1373                     )\n1374             elif hasattr(val, 'prepare_database_save'):\n1375                 if field.remote_field:\n1376                     val = field.get_db_prep_save(\n1377                         val.prepare_database_save(field),\n1378                         connection=self.connection,\n1379                     )\n1380                 else:\n1381                     raise TypeError(\n1382                         \"Tried to update field %s with a model instance, %r. \"\n1383                         \"Use a value compatible with %s.\"\n1384                         % (field, val, field.__class__.__name__)\n1385                     )\n1386             else:\n1387                 val = field.get_db_prep_save(val, connection=self.connection)\n1388 \n1389             # Getting the placeholder for the field.\n1390             if hasattr(field, 'get_placeholder'):\n1391                 placeholder = field.get_placeholder(val, self, self.connection)\n1392             else:\n1393                 placeholder = '%s'\n1394             name = field.column\n1395             if hasattr(val, 'as_sql'):\n1396                 sql, params = self.compile(val)\n1397                 values.append('%s = %s' % (qn(name), placeholder % sql))\n1398                 update_params.extend(params)\n1399             elif val is not None:\n1400                 values.append('%s = %s' % (qn(name), placeholder))\n1401                 update_params.append(val)\n1402             else:\n1403                 values.append('%s = NULL' % qn(name))\n1404         table = self.query.base_table\n1405         result = [\n1406             'UPDATE %s SET' % qn(table),\n1407             ', '.join(values),\n1408         ]\n1409         where, params = self.compile(self.query.where)\n1410         if where:\n1411             result.append('WHERE %s' % where)\n1412         return ' '.join(result), tuple(update_params + params)\n1413 \n1414     def execute_sql(self, result_type):\n1415         \"\"\"\n1416         Execute the specified update. Return the number of rows affected by\n1417         the primary update query. The \"primary update query\" is the first\n1418         non-empty query that is executed. Row counts for any subsequent,\n1419         related queries are not available.\n1420         \"\"\"\n1421         cursor = super().execute_sql(result_type)\n1422         try:\n1423             rows = cursor.rowcount if cursor else 0\n1424             is_empty = cursor is None\n1425         finally:\n1426             if cursor:\n1427                 cursor.close()\n1428         for query in self.query.get_related_updates():\n1429             aux_rows = query.get_compiler(self.using).execute_sql(result_type)\n1430             if is_empty and aux_rows:\n1431                 rows = aux_rows\n1432                 is_empty = False\n1433         return rows\n1434 \n1435     def pre_sql_setup(self):\n1436         \"\"\"\n1437         If the update depends on results from other tables, munge the \"where\"\n1438         conditions to match the format required for (portable) SQL updates.\n1439 \n1440         If multiple updates are required, pull out the id values to update at\n1441         this point so that they don't change as a result of the progressive\n1442         updates.\n1443         \"\"\"\n1444         refcounts_before = self.query.alias_refcount.copy()\n1445         # Ensure base table is in the query\n1446         self.query.get_initial_alias()\n1447         count = self.query.count_active_tables()\n1448         if not self.query.related_updates and count == 1:\n1449             return\n1450         query = self.query.chain(klass=Query)\n1451         query.select_related = False\n1452         query.clear_ordering(True)\n1453         query.extra = {}\n1454         query.select = []\n1455         query.add_fields([query.get_meta().pk.name])\n1456         super().pre_sql_setup()\n1457 \n1458         must_pre_select = count > 1 and not self.connection.features.update_can_self_select\n1459 \n1460         # Now we adjust the current query: reset the where clause and get rid\n1461         # of all the tables we don't need (since they're in the sub-select).\n1462         self.query.where = self.query.where_class()\n1463         if self.query.related_updates or must_pre_select:\n1464             # Either we're using the idents in multiple update queries (so\n1465             # don't want them to change), or the db backend doesn't support\n1466             # selecting from the updating table (e.g. MySQL).\n1467             idents = []\n1468             for rows in query.get_compiler(self.using).execute_sql(MULTI):\n1469                 idents.extend(r[0] for r in rows)\n1470             self.query.add_filter(('pk__in', idents))\n1471             self.query.related_ids = idents\n1472         else:\n1473             # The fast path. Filters and updates in one query.\n1474             self.query.add_filter(('pk__in', query))\n1475         self.query.reset_refcounts(refcounts_before)\n1476 \n1477 \n1478 class SQLAggregateCompiler(SQLCompiler):\n1479     def as_sql(self):\n1480         \"\"\"\n1481         Create the SQL for this query. Return the SQL string and list of\n1482         parameters.\n1483         \"\"\"\n1484         sql, params = [], []\n1485         for annotation in self.query.annotation_select.values():\n1486             ann_sql, ann_params = self.compile(annotation, select_format=FORCE)\n1487             sql.append(ann_sql)\n1488             params.extend(ann_params)\n1489         self.col_count = len(self.query.annotation_select)\n1490         sql = ', '.join(sql)\n1491         params = tuple(params)\n1492 \n1493         sql = 'SELECT %s FROM (%s) subquery' % (sql, self.query.subquery)\n1494         params = params + self.query.sub_params\n1495         return sql, params\n1496 \n1497 \n1498 def cursor_iter(cursor, sentinel, col_count, itersize):\n1499     \"\"\"\n1500     Yield blocks of rows from a cursor and ensure the cursor is closed when\n1501     done.\n1502     \"\"\"\n1503     try:\n1504         for rows in iter((lambda: cursor.fetchmany(itersize)), sentinel):\n1505             yield rows if col_count is None else [r[:col_count] for r in rows]\n1506     finally:\n1507         cursor.close()\n1508 \n[end of django/db/models/sql/compiler.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n"
}