{
  "instance_id": "astropy__astropy-6938",
  "variant": "oracle",
  "split": "test",
  "meta": {
    "instance_id": "astropy__astropy-6938",
    "repo": "astropy/astropy",
    "base_commit": "c76af9ed6bb89bfba45b9f5bc1e635188278e2fa",
    "patch": "<patch>\ndiff --git a/astropy/io/fits/fitsrec.py b/astropy/io/fits/fitsrec.py\n--- a/astropy/io/fits/fitsrec.py\n+++ b/astropy/io/fits/fitsrec.py\n@@ -1261,7 +1261,7 @@ def _scale_back_ascii(self, col_idx, input_field, output_field):\n \n         # Replace exponent separator in floating point numbers\n         if 'D' in format:\n-            output_field.replace(encode_ascii('E'), encode_ascii('D'))\n+            output_field[:] = output_field.replace(b'E', b'D')\n \n \n def _get_recarray_field(array, key):\n\n</patch>",
    "test_patch": "diff --git a/astropy/io/fits/tests/test_checksum.py b/astropy/io/fits/tests/test_checksum.py\n--- a/astropy/io/fits/tests/test_checksum.py\n+++ b/astropy/io/fits/tests/test_checksum.py\n@@ -205,9 +205,9 @@ def test_ascii_table_data(self):\n                 # The checksum ends up being different on Windows, possibly due\n                 # to slight floating point differences\n                 assert 'CHECKSUM' in hdul[1].header\n-                assert hdul[1].header['CHECKSUM'] == '51IDA1G981GCA1G9'\n+                assert hdul[1].header['CHECKSUM'] == '3rKFAoI94oICAoI9'\n                 assert 'DATASUM' in hdul[1].header\n-                assert hdul[1].header['DATASUM'] == '1948208413'\n+                assert hdul[1].header['DATASUM'] == '1914653725'\n \n     def test_compressed_image_data(self):\n         with fits.open(self.data('comp.fits')) as h1:\ndiff --git a/astropy/io/fits/tests/test_table.py b/astropy/io/fits/tests/test_table.py\n--- a/astropy/io/fits/tests/test_table.py\n+++ b/astropy/io/fits/tests/test_table.py\n@@ -298,6 +298,19 @@ def test_ascii_table(self):\n         hdul = fits.open(self.temp('toto.fits'))\n         assert comparerecords(hdu.data, hdul[1].data)\n         hdul.close()\n+\n+        # Test Scaling\n+\n+        r1 = np.array([11., 12.])\n+        c2 = fits.Column(name='def', format='D', array=r1, bscale=2.3,\n+                         bzero=0.6)\n+        hdu = fits.TableHDU.from_columns([c2])\n+        hdu.writeto(self.temp('toto.fits'), overwrite=True)\n+        with open(self.temp('toto.fits')) as f:\n+            assert '4.95652173913043548D+00' in f.read()\n+        with fits.open(self.temp('toto.fits')) as hdul:\n+            assert comparerecords(hdu.data, hdul[1].data)\n+\n         a.close()\n \n     def test_endianness(self):\n",
    "created_at": "2017-12-07T00:01:14Z",
    "version": "1.3",
    "FAIL_TO_PASS": "[\"astropy/io/fits/tests/test_checksum.py::TestChecksumFunctions::test_ascii_table_data\", \"astropy/io/fits/tests/test_table.py::TestTableFunctions::test_ascii_table\"]",
    "PASS_TO_PASS": "[\"astropy/io/fits/tests/test_checksum.py::TestChecksumFunctions::test_sample_file\", \"astropy/io/fits/tests/test_checksum.py::TestChecksumFunctions::test_image_create\", \"astropy/io/fits/tests/test_checksum.py::TestChecksumFunctions::test_scaled_data\", \"astropy/io/fits/tests/test_checksum.py::TestChecksumFunctions::test_scaled_data_auto_rescale\", \"astropy/io/fits/tests/test_checksum.py::TestChecksumFunctions::test_uint16_data\", \"astropy/io/fits/tests/test_checksum.py::TestChecksumFunctions::test_groups_hdu_data\", \"astropy/io/fits/tests/test_checksum.py::TestChecksumFunctions::test_open_with_no_keywords\", \"astropy/io/fits/tests/test_checksum.py::TestChecksumFunctions::test_writeto_convenience\", \"astropy/io/fits/tests/test_checksum.py::TestChecksumFunctions::test_hdu_writeto\", \"astropy/io/fits/tests/test_checksum.py::TestChecksumFunctions::test_datasum_only\", \"astropy/io/fits/tests/test_table.py::test_regression_scalar_indexing\"]"
  },
  "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nPossible bug in io.fits related to D exponents\nI came across the following code in ``fitsrec.py``:\r\n\r\n```python\r\n        # Replace exponent separator in floating point numbers\r\n        if 'D' in format:\r\n            output_field.replace(encode_ascii('E'), encode_ascii('D'))\r\n```\r\n\r\nI think this may be incorrect because as far as I can tell ``replace`` is not an in-place operation for ``chararray`` (it returns a copy). Commenting out this code doesn't cause any tests to fail so I think this code isn't being tested anyway.\n\n</issue>\n<code>\n[start of README.rst]\n1 =======\n2 Astropy\n3 =======\n4 \n5 .. image:: https://img.shields.io/pypi/v/astropy.svg\n6     :target: https://pypi.python.org/pypi/astropy\n7 \n8 Astropy (http://www.astropy.org) is a package intended to contain much of\n9 the core functionality and some common tools needed for performing\n10 astronomy and astrophysics with Python.\n11 \n12 Releases are `registered on PyPI <http://pypi.python.org/pypi/astropy>`_,\n13 and development is occurring at the\n14 `project's github page <http://github.com/astropy/astropy>`_.\n15 \n16 For installation instructions, see the `online documentation <http://docs.astropy.org/>`_\n17 or  ``docs/install.rst`` in this source distribution.\n18 \n19 For system packagers: Please install Astropy with the command::\n20 \n21     $ python setup.py --offline install\n22 \n23 This will prevent the astropy_helpers bootstrap script from attempting to\n24 reach out to PyPI.\n25 \n26 Project Status\n27 --------------\n28 \n29 .. image:: https://travis-ci.org/astropy/astropy.svg\n30     :target: https://travis-ci.org/astropy/astropy\n31     :alt: Astropy's Travis CI Status\n32 \n33 .. image:: https://coveralls.io/repos/astropy/astropy/badge.svg\n34     :target: https://coveralls.io/r/astropy/astropy\n35     :alt: Astropy's Coveralls Status\n36 \n37 .. image:: https://ci.appveyor.com/api/projects/status/ym7lxajcs5qwm31e/branch/master?svg=true\n38     :target: https://ci.appveyor.com/project/Astropy/astropy/branch/master\n39     :alt: Astropy's Appveyor Status\n40 \n41 For an overview of the testing and build status of all packages associated\n42 with the Astropy Project, see http://dashboard.astropy.org.\n43 \n44 .. image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A\n45     :target: http://numfocus.org\n46     :alt: Powered by NumFOCUS\n47 \n48 \n49 Contributing Code, Documentation, or Feedback\n50 ---------------------------------------------\n51 The Astropy project is made both by and for its users, so we welcome and encourage\n52 contributions of many kinds. Our goal is to keep this a positive, inclusive,\n53 successful, and growing community, by abiding with the\n54 `Astropy Community Code of Conduct <http://www.astropy.org/about.html#codeofconduct>`_.\n55 \n56 More detailed information on contributing to the project or submitting feedback\n57 can be found on the `contributions <http://www.astropy.org/contribute.html>`_ page.\n58 \n59 A `summary of contribution guidelines <CONTRIBUTING.md>`_ can also be used as a quick\n60 reference when you're ready to start writing or validating code for submission.\n61 \n62 License\n63 -------\n64 Astropy is licensed under a 3-clause BSD style license - see the\n65 ``LICENSE.rst`` file.\n66 \n[end of README.rst]\n[start of astropy/io/fits/fitsrec.py]\n1 # Licensed under a 3-clause BSD style license - see PYFITS.rst\n2 \n3 import copy\n4 import operator\n5 import warnings\n6 import weakref\n7 \n8 from contextlib import suppress\n9 from functools import reduce\n10 \n11 import numpy as np\n12 \n13 from numpy import char as chararray\n14 \n15 from .column import (ASCIITNULL, FITS2NUMPY, ASCII2NUMPY, ASCII2STR, ColDefs,\n16                      _AsciiColDefs, _FormatX, _FormatP, _VLF, _get_index,\n17                      _wrapx, _unwrapx, _makep, Delayed)\n18 from .util import decode_ascii, encode_ascii, _rstrip_inplace\n19 from ...utils import lazyproperty\n20 \n21 \n22 class FITS_record:\n23     \"\"\"\n24     FITS record class.\n25 \n26     `FITS_record` is used to access records of the `FITS_rec` object.\n27     This will allow us to deal with scaled columns.  It also handles\n28     conversion/scaling of columns in ASCII tables.  The `FITS_record`\n29     class expects a `FITS_rec` object as input.\n30     \"\"\"\n31 \n32     def __init__(self, input, row=0, start=None, end=None, step=None,\n33                  base=None, **kwargs):\n34         \"\"\"\n35         Parameters\n36         ----------\n37         input : array\n38            The array to wrap.\n39 \n40         row : int, optional\n41            The starting logical row of the array.\n42 \n43         start : int, optional\n44            The starting column in the row associated with this object.\n45            Used for subsetting the columns of the `FITS_rec` object.\n46 \n47         end : int, optional\n48            The ending column in the row associated with this object.\n49            Used for subsetting the columns of the `FITS_rec` object.\n50         \"\"\"\n51 \n52         self.array = input\n53         self.row = row\n54         if base:\n55             width = len(base)\n56         else:\n57             width = self.array._nfields\n58 \n59         s = slice(start, end, step).indices(width)\n60         self.start, self.end, self.step = s\n61         self.base = base\n62 \n63     def __getitem__(self, key):\n64         if isinstance(key, str):\n65             indx = _get_index(self.array.names, key)\n66 \n67             if indx < self.start or indx > self.end - 1:\n68                 raise KeyError(\"Key '{}' does not exist.\".format(key))\n69         elif isinstance(key, slice):\n70             return type(self)(self.array, self.row, key.start, key.stop,\n71                               key.step, self)\n72         else:\n73             indx = self._get_index(key)\n74 \n75             if indx > self.array._nfields - 1:\n76                 raise IndexError('Index out of bounds')\n77 \n78         return self.array.field(indx)[self.row]\n79 \n80     def __setitem__(self, key, value):\n81         if isinstance(key, str):\n82             indx = _get_index(self.array.names, key)\n83 \n84             if indx < self.start or indx > self.end - 1:\n85                 raise KeyError(\"Key '{}' does not exist.\".format(key))\n86         elif isinstance(key, slice):\n87             for indx in range(slice.start, slice.stop, slice.step):\n88                 indx = self._get_indx(indx)\n89                 self.array.field(indx)[self.row] = value\n90         else:\n91             indx = self._get_index(key)\n92             if indx > self.array._nfields - 1:\n93                 raise IndexError('Index out of bounds')\n94 \n95         self.array.field(indx)[self.row] = value\n96 \n97     def __len__(self):\n98         return len(range(self.start, self.end, self.step))\n99 \n100     def __repr__(self):\n101         \"\"\"\n102         Display a single row.\n103         \"\"\"\n104 \n105         outlist = []\n106         for idx in range(len(self)):\n107             outlist.append(repr(self[idx]))\n108         return '({})'.format(', '.join(outlist))\n109 \n110     def field(self, field):\n111         \"\"\"\n112         Get the field data of the record.\n113         \"\"\"\n114 \n115         return self.__getitem__(field)\n116 \n117     def setfield(self, field, value):\n118         \"\"\"\n119         Set the field data of the record.\n120         \"\"\"\n121 \n122         self.__setitem__(field, value)\n123 \n124     @lazyproperty\n125     def _bases(self):\n126         bases = [weakref.proxy(self)]\n127         base = self.base\n128         while base:\n129             bases.append(base)\n130             base = base.base\n131         return bases\n132 \n133     def _get_index(self, indx):\n134         indices = np.ogrid[:self.array._nfields]\n135         for base in reversed(self._bases):\n136             if base.step < 1:\n137                 s = slice(base.start, None, base.step)\n138             else:\n139                 s = slice(base.start, base.end, base.step)\n140             indices = indices[s]\n141         return indices[indx]\n142 \n143 \n144 class FITS_rec(np.recarray):\n145     \"\"\"\n146     FITS record array class.\n147 \n148     `FITS_rec` is the data part of a table HDU's data part.  This is a layer\n149     over the `~numpy.recarray`, so we can deal with scaled columns.\n150 \n151     It inherits all of the standard methods from `numpy.ndarray`.\n152     \"\"\"\n153 \n154     _record_type = FITS_record\n155     _character_as_bytes = False\n156 \n157     def __new__(subtype, input):\n158         \"\"\"\n159         Construct a FITS record array from a recarray.\n160         \"\"\"\n161 \n162         # input should be a record array\n163         if input.dtype.subdtype is None:\n164             self = np.recarray.__new__(subtype, input.shape, input.dtype,\n165                                        buf=input.data)\n166         else:\n167             self = np.recarray.__new__(subtype, input.shape, input.dtype,\n168                                        buf=input.data, strides=input.strides)\n169 \n170         self._init()\n171         if self.dtype.fields:\n172             self._nfields = len(self.dtype.fields)\n173 \n174         return self\n175 \n176     def __setstate__(self, state):\n177         meta = state[-1]\n178         column_state = state[-2]\n179         state = state[:-2]\n180 \n181         super().__setstate__(state)\n182 \n183         self._col_weakrefs = weakref.WeakSet()\n184 \n185         for attr, value in zip(meta, column_state):\n186             setattr(self, attr, value)\n187 \n188     def __reduce__(self):\n189         \"\"\"\n190         Return a 3-tuple for pickling a FITS_rec. Use the super-class\n191         functionality but then add in a tuple of FITS_rec-specific\n192         values that get used in __setstate__.\n193         \"\"\"\n194 \n195         reconst_func, reconst_func_args, state = super().__reduce__()\n196 \n197         # Define FITS_rec-specific attrs that get added to state\n198         column_state = []\n199         meta = []\n200 \n201         for attrs in ['_converted', '_heapoffset', '_heapsize', '_nfields',\n202                       '_gap', '_uint', 'parnames', '_coldefs']:\n203 \n204             with suppress(AttributeError):\n205                 # _coldefs can be Delayed, and file objects cannot be\n206                 # picked, it needs to be deepcopied first\n207                 if attrs == '_coldefs':\n208                     column_state.append(self._coldefs.__deepcopy__(None))\n209                 else:\n210                     column_state.append(getattr(self, attrs))\n211                 meta.append(attrs)\n212 \n213         state = state + (column_state, meta)\n214 \n215         return reconst_func, reconst_func_args, state\n216 \n217     def __array_finalize__(self, obj):\n218         if obj is None:\n219             return\n220 \n221         if isinstance(obj, FITS_rec):\n222             self._character_as_bytes = obj._character_as_bytes\n223 \n224         if isinstance(obj, FITS_rec) and obj.dtype == self.dtype:\n225             self._converted = obj._converted\n226             self._heapoffset = obj._heapoffset\n227             self._heapsize = obj._heapsize\n228             self._col_weakrefs = obj._col_weakrefs\n229             self._coldefs = obj._coldefs\n230             self._nfields = obj._nfields\n231             self._gap = obj._gap\n232             self._uint = obj._uint\n233         elif self.dtype.fields is not None:\n234             # This will allow regular ndarrays with fields, rather than\n235             # just other FITS_rec objects\n236             self._nfields = len(self.dtype.fields)\n237             self._converted = {}\n238 \n239             self._heapoffset = getattr(obj, '_heapoffset', 0)\n240             self._heapsize = getattr(obj, '_heapsize', 0)\n241 \n242             self._gap = getattr(obj, '_gap', 0)\n243             self._uint = getattr(obj, '_uint', False)\n244             self._col_weakrefs = weakref.WeakSet()\n245             self._coldefs = ColDefs(self)\n246 \n247             # Work around chicken-egg problem.  Column.array relies on the\n248             # _coldefs attribute to set up ref back to parent FITS_rec; however\n249             # in the above line the self._coldefs has not been assigned yet so\n250             # this fails.  This patches that up...\n251             for col in self._coldefs:\n252                 del col.array\n253                 col._parent_fits_rec = weakref.ref(self)\n254         else:\n255             self._init()\n256 \n257     def _init(self):\n258         \"\"\"Initializes internal attributes specific to FITS-isms.\"\"\"\n259 \n260         self._nfields = 0\n261         self._converted = {}\n262         self._heapoffset = 0\n263         self._heapsize = 0\n264         self._col_weakrefs = weakref.WeakSet()\n265         self._coldefs = None\n266         self._gap = 0\n267         self._uint = False\n268 \n269     @classmethod\n270     def from_columns(cls, columns, nrows=0, fill=False, character_as_bytes=False):\n271         \"\"\"\n272         Given a `ColDefs` object of unknown origin, initialize a new `FITS_rec`\n273         object.\n274 \n275         .. note::\n276 \n277             This was originally part of the ``new_table`` function in the table\n278             module but was moved into a class method since most of its\n279             functionality always had more to do with initializing a `FITS_rec`\n280             object than anything else, and much of it also overlapped with\n281             ``FITS_rec._scale_back``.\n282 \n283         Parameters\n284         ----------\n285         columns : sequence of `Column` or a `ColDefs`\n286             The columns from which to create the table data.  If these\n287             columns have data arrays attached that data may be used in\n288             initializing the new table.  Otherwise the input columns\n289             will be used as a template for a new table with the requested\n290             number of rows.\n291 \n292         nrows : int\n293             Number of rows in the new table.  If the input columns have data\n294             associated with them, the size of the largest input column is used.\n295             Otherwise the default is 0.\n296 \n297         fill : bool\n298             If `True`, will fill all cells with zeros or blanks.  If\n299             `False`, copy the data from input, undefined cells will still\n300             be filled with zeros/blanks.\n301         \"\"\"\n302 \n303         if not isinstance(columns, ColDefs):\n304             columns = ColDefs(columns)\n305 \n306         # read the delayed data\n307         for column in columns:\n308             arr = column.array\n309             if isinstance(arr, Delayed):\n310                 if arr.hdu.data is None:\n311                     column.array = None\n312                 else:\n313                     column.array = _get_recarray_field(arr.hdu.data,\n314                                                        arr.field)\n315         # Reset columns._arrays (which we may want to just do away with\n316         # altogether\n317         del columns._arrays\n318 \n319         # use the largest column shape as the shape of the record\n320         if nrows == 0:\n321             for arr in columns._arrays:\n322                 if arr is not None:\n323                     dim = arr.shape[0]\n324                 else:\n325                     dim = 0\n326                 if dim > nrows:\n327                     nrows = dim\n328 \n329         raw_data = np.empty(columns.dtype.itemsize * nrows, dtype=np.uint8)\n330         raw_data.fill(ord(columns._padding_byte))\n331         data = np.recarray(nrows, dtype=columns.dtype, buf=raw_data).view(cls)\n332         data._character_as_bytes = character_as_bytes\n333 \n334         # Make sure the data is a listener for changes to the columns\n335         columns._add_listener(data)\n336 \n337         # Previously this assignment was made from hdu.columns, but that's a\n338         # bug since if a _TableBaseHDU has a FITS_rec in its .data attribute\n339         # the _TableBaseHDU.columns property is actually returned from\n340         # .data._coldefs, so this assignment was circular!  Don't make that\n341         # mistake again.\n342         # All of this is an artifact of the fragility of the FITS_rec class,\n343         # and that it can't just be initialized by columns...\n344         data._coldefs = columns\n345 \n346         # If fill is True we don't copy anything from the column arrays.  We're\n347         # just using them as a template, and returning a table filled with\n348         # zeros/blanks\n349         if fill:\n350             return data\n351 \n352         # Otherwise we have to fill the recarray with data from the input\n353         # columns\n354         for idx, column in enumerate(columns):\n355             # For each column in the ColDef object, determine the number of\n356             # rows in that column.  This will be either the number of rows in\n357             # the ndarray associated with the column, or the number of rows\n358             # given in the call to this function, which ever is smaller.  If\n359             # the input FILL argument is true, the number of rows is set to\n360             # zero so that no data is copied from the original input data.\n361             arr = column.array\n362 \n363             if arr is None:\n364                 array_size = 0\n365             else:\n366                 array_size = len(arr)\n367 \n368             n = min(array_size, nrows)\n369 \n370             # TODO: At least *some* of this logic is mostly redundant with the\n371             # _convert_foo methods in this class; see if we can eliminate some\n372             # of that duplication.\n373 \n374             if not n:\n375                 # The input column had an empty array, so just use the fill\n376                 # value\n377                 continue\n378 \n379             field = _get_recarray_field(data, idx)\n380             name = column.name\n381             fitsformat = column.format\n382             recformat = fitsformat.recformat\n383 \n384             outarr = field[:n]\n385             inarr = arr[:n]\n386 \n387             if isinstance(recformat, _FormatX):\n388                 # Data is a bit array\n389                 if inarr.shape[-1] == recformat.repeat:\n390                     _wrapx(inarr, outarr, recformat.repeat)\n391                     continue\n392             elif isinstance(recformat, _FormatP):\n393                 data._cache_field(name, _makep(inarr, field, recformat,\n394                                                nrows=nrows))\n395                 continue\n396             # TODO: Find a better way of determining that the column is meant\n397             # to be FITS L formatted\n398             elif recformat[-2:] == FITS2NUMPY['L'] and inarr.dtype == bool:\n399                 # column is boolean\n400                 # The raw data field should be filled with either 'T' or 'F'\n401                 # (not 0).  Use 'F' as a default\n402                 field[:] = ord('F')\n403                 # Also save the original boolean array in data._converted so\n404                 # that it doesn't have to be re-converted\n405                 converted = np.zeros(field.shape, dtype=bool)\n406                 converted[:n] = inarr\n407                 data._cache_field(name, converted)\n408                 # TODO: Maybe this step isn't necessary at all if _scale_back\n409                 # will handle it?\n410                 inarr = np.where(inarr == np.False_, ord('F'), ord('T'))\n411             elif (columns[idx]._physical_values and\n412                     columns[idx]._pseudo_unsigned_ints):\n413                 # Temporary hack...\n414                 bzero = column.bzero\n415                 converted = np.zeros(field.shape, dtype=inarr.dtype)\n416                 converted[:n] = inarr\n417                 data._cache_field(name, converted)\n418                 if n < nrows:\n419                     # Pre-scale rows below the input data\n420                     field[n:] = -bzero\n421 \n422                 inarr = inarr - bzero\n423             elif isinstance(columns, _AsciiColDefs):\n424                 # Regardless whether the format is character or numeric, if the\n425                 # input array contains characters then it's already in the raw\n426                 # format for ASCII tables\n427                 if fitsformat._pseudo_logical:\n428                     # Hack to support converting from 8-bit T/F characters\n429                     # Normally the column array is a chararray of 1 character\n430                     # strings, but we need to view it as a normal ndarray of\n431                     # 8-bit ints to fill it with ASCII codes for 'T' and 'F'\n432                     outarr = field.view(np.uint8, np.ndarray)[:n]\n433                 elif arr.dtype.kind not in ('S', 'U'):\n434                     # Set up views of numeric columns with the appropriate\n435                     # numeric dtype\n436                     # Fill with the appropriate blanks for the column format\n437                     data._cache_field(name, np.zeros(nrows, dtype=arr.dtype))\n438                     outarr = data._converted[name][:n]\n439 \n440                 outarr[:] = inarr\n441                 continue\n442 \n443             if inarr.shape != outarr.shape:\n444                 if (inarr.dtype.kind == outarr.dtype.kind and\n445                         inarr.dtype.kind in ('U', 'S') and\n446                         inarr.dtype != outarr.dtype):\n447 \n448                     inarr_rowsize = inarr[0].size\n449                     inarr = inarr.flatten().view(outarr.dtype)\n450 \n451                 # This is a special case to handle input arrays with\n452                 # non-trivial TDIMn.\n453                 # By design each row of the outarray is 1-D, while each row of\n454                 # the input array may be n-D\n455                 if outarr.ndim > 1:\n456                     # The normal case where the first dimension is the rows\n457                     inarr_rowsize = inarr[0].size\n458                     inarr = inarr.reshape(n, inarr_rowsize)\n459                     outarr[:, :inarr_rowsize] = inarr\n460                 else:\n461                     # Special case for strings where the out array only has one\n462                     # dimension (the second dimension is rolled up into the\n463                     # strings\n464                     outarr[:n] = inarr.ravel()\n465             else:\n466                 outarr[:] = inarr\n467 \n468         # Now replace the original column array references with the new\n469         # fields\n470         # This is required to prevent the issue reported in\n471         # https://github.com/spacetelescope/PyFITS/issues/99\n472         for idx in range(len(columns)):\n473             columns._arrays[idx] = data.field(idx)\n474 \n475         return data\n476 \n477     def __repr__(self):\n478         # Force use of the normal ndarray repr (rather than the new\n479         # one added for recarray in Numpy 1.10) for backwards compat\n480         return np.ndarray.__repr__(self)\n481 \n482     def __getitem__(self, key):\n483         if self._coldefs is None:\n484             return super().__getitem__(key)\n485 \n486         if isinstance(key, str):\n487             return self.field(key)\n488 \n489         # Have to view as a recarray then back as a FITS_rec, otherwise the\n490         # circular reference fix/hack in FITS_rec.field() won't preserve\n491         # the slice.\n492         out = self.view(np.recarray)[key]\n493         if type(out) is not np.recarray:\n494             # Oops, we got a single element rather than a view. In that case,\n495             # return a Record, which has no __getstate__ and is more efficient.\n496             return self._record_type(self, key)\n497 \n498         # We got a view; change it back to our class, and add stuff\n499         out = out.view(type(self))\n500         out._coldefs = ColDefs(self._coldefs)\n501         arrays = []\n502         out._converted = {}\n503         for idx, name in enumerate(self._coldefs.names):\n504             #\n505             # Store the new arrays for the _coldefs object\n506             #\n507             arrays.append(self._coldefs._arrays[idx][key])\n508 \n509             # Ensure that the sliced FITS_rec will view the same scaled\n510             # columns as the original; this is one of the few cases where\n511             # it is not necessary to use _cache_field()\n512             if name in self._converted:\n513                 dummy = self._converted[name]\n514                 field = np.ndarray.__getitem__(dummy, key)\n515                 out._converted[name] = field\n516 \n517         out._coldefs._arrays = arrays\n518         return out\n519 \n520     def __setitem__(self, key, value):\n521         if self._coldefs is None:\n522             return super().__setitem__(key, value)\n523 \n524         if isinstance(key, str):\n525             self[key][:] = value\n526             return\n527 \n528         if isinstance(key, slice):\n529             end = min(len(self), key.stop or len(self))\n530             end = max(0, end)\n531             start = max(0, key.start or 0)\n532             end = min(end, start + len(value))\n533 \n534             for idx in range(start, end):\n535                 self.__setitem__(idx, value[idx - start])\n536             return\n537 \n538         if isinstance(value, FITS_record):\n539             for idx in range(self._nfields):\n540                 self.field(self.names[idx])[key] = value.field(self.names[idx])\n541         elif isinstance(value, (tuple, list, np.void)):\n542             if self._nfields == len(value):\n543                 for idx in range(self._nfields):\n544                     self.field(idx)[key] = value[idx]\n545             else:\n546                 raise ValueError('Input tuple or list required to have {} '\n547                                  'elements.'.format(self._nfields))\n548         else:\n549             raise TypeError('Assignment requires a FITS_record, tuple, or '\n550                             'list as input.')\n551 \n552     def copy(self, order='C'):\n553         \"\"\"\n554         The Numpy documentation lies; `numpy.ndarray.copy` is not equivalent to\n555         `numpy.copy`.  Differences include that it re-views the copied array as\n556         self's ndarray subclass, as though it were taking a slice; this means\n557         ``__array_finalize__`` is called and the copy shares all the array\n558         attributes (including ``._converted``!).  So we need to make a deep\n559         copy of all those attributes so that the two arrays truly do not share\n560         any data.\n561         \"\"\"\n562 \n563         new = super().copy(order=order)\n564 \n565         new.__dict__ = copy.deepcopy(self.__dict__)\n566         return new\n567 \n568     @property\n569     def columns(self):\n570         \"\"\"\n571         A user-visible accessor for the coldefs.\n572 \n573         See https://aeon.stsci.edu/ssb/trac/pyfits/ticket/44\n574         \"\"\"\n575 \n576         return self._coldefs\n577 \n578     @property\n579     def _coldefs(self):\n580         # This used to be a normal internal attribute, but it was changed to a\n581         # property as a quick and transparent way to work around the reference\n582         # leak bug fixed in https://github.com/astropy/astropy/pull/4539\n583         #\n584         # See the long comment in the Column.array property for more details\n585         # on this.  But in short, FITS_rec now has a ._col_weakrefs attribute\n586         # which is a WeakSet of weakrefs to each Column in _coldefs.\n587         #\n588         # So whenever ._coldefs is set we also add each Column in the ColDefs\n589         # to the weakrefs set.  This is an easy way to find out if a Column has\n590         # any references to it external to the FITS_rec (i.e. a user assigned a\n591         # column to a variable).  If the column is still in _col_weakrefs then\n592         # there are other references to it external to this FITS_rec.  We use\n593         # that information in __del__ to save off copies of the array data\n594         # for those columns to their Column.array property before our memory\n595         # is freed.\n596         return self.__dict__.get('_coldefs')\n597 \n598     @_coldefs.setter\n599     def _coldefs(self, cols):\n600         self.__dict__['_coldefs'] = cols\n601         if isinstance(cols, ColDefs):\n602             for col in cols.columns:\n603                 self._col_weakrefs.add(col)\n604 \n605     @_coldefs.deleter\n606     def _coldefs(self):\n607         try:\n608             del self.__dict__['_coldefs']\n609         except KeyError as exc:\n610             raise AttributeError(exc.args[0])\n611 \n612     def __del__(self):\n613         try:\n614             del self._coldefs\n615             if self.dtype.fields is not None:\n616                 for col in self._col_weakrefs:\n617 \n618                     if col.array is not None:\n619                         col.array = col.array.copy()\n620 \n621         # See issues #4690 and #4912\n622         except (AttributeError, TypeError):  # pragma: no cover\n623             pass\n624 \n625     @property\n626     def names(self):\n627         \"\"\"List of column names.\"\"\"\n628 \n629         if self.dtype.fields:\n630             return list(self.dtype.names)\n631         elif getattr(self, '_coldefs', None) is not None:\n632             return self._coldefs.names\n633         else:\n634             return None\n635 \n636     @property\n637     def formats(self):\n638         \"\"\"List of column FITS formats.\"\"\"\n639 \n640         if getattr(self, '_coldefs', None) is not None:\n641             return self._coldefs.formats\n642 \n643         return None\n644 \n645     @property\n646     def _raw_itemsize(self):\n647         \"\"\"\n648         Returns the size of row items that would be written to the raw FITS\n649         file, taking into account the possibility of unicode columns being\n650         compactified.\n651 \n652         Currently for internal use only.\n653         \"\"\"\n654 \n655         if _has_unicode_fields(self):\n656             total_itemsize = 0\n657             for field in self.dtype.fields.values():\n658                 itemsize = field[0].itemsize\n659                 if field[0].kind == 'U':\n660                     itemsize = itemsize // 4\n661                 total_itemsize += itemsize\n662             return total_itemsize\n663         else:\n664             # Just return the normal itemsize\n665             return self.itemsize\n666 \n667     def field(self, key):\n668         \"\"\"\n669         A view of a `Column`'s data as an array.\n670         \"\"\"\n671 \n672         # NOTE: The *column* index may not be the same as the field index in\n673         # the recarray, if the column is a phantom column\n674         column = self.columns[key]\n675         name = column.name\n676         format = column.format\n677 \n678         if format.dtype.itemsize == 0:\n679             warnings.warn(\n680                 'Field {!r} has a repeat count of 0 in its format code, '\n681                 'indicating an empty field.'.format(key))\n682             return np.array([], dtype=format.dtype)\n683 \n684         # If field's base is a FITS_rec, we can run into trouble because it\n685         # contains a reference to the ._coldefs object of the original data;\n686         # this can lead to a circular reference; see ticket #49\n687         base = self\n688         while (isinstance(base, FITS_rec) and\n689                 isinstance(base.base, np.recarray)):\n690             base = base.base\n691         # base could still be a FITS_rec in some cases, so take care to\n692         # use rec.recarray.field to avoid a potential infinite\n693         # recursion\n694         field = _get_recarray_field(base, name)\n695 \n696         if name not in self._converted:\n697             recformat = format.recformat\n698             # TODO: If we're now passing the column to these subroutines, do we\n699             # really need to pass them the recformat?\n700             if isinstance(recformat, _FormatP):\n701                 # for P format\n702                 converted = self._convert_p(column, field, recformat)\n703             else:\n704                 # Handle all other column data types which are fixed-width\n705                 # fields\n706                 converted = self._convert_other(column, field, recformat)\n707 \n708             # Note: Never assign values directly into the self._converted dict;\n709             # always go through self._cache_field; this way self._converted is\n710             # only used to store arrays that are not already direct views of\n711             # our own data.\n712             self._cache_field(name, converted)\n713             return converted\n714 \n715         return self._converted[name]\n716 \n717     def _cache_field(self, name, field):\n718         \"\"\"\n719         Do not store fields in _converted if one of its bases is self,\n720         or if it has a common base with self.\n721 \n722         This results in a reference cycle that cannot be broken since\n723         ndarrays do not participate in cyclic garbage collection.\n724         \"\"\"\n725 \n726         base = field\n727         while True:\n728             self_base = self\n729             while True:\n730                 if self_base is base:\n731                     return\n732 \n733                 if getattr(self_base, 'base', None) is not None:\n734                     self_base = self_base.base\n735                 else:\n736                     break\n737 \n738             if getattr(base, 'base', None) is not None:\n739                 base = base.base\n740             else:\n741                 break\n742 \n743         self._converted[name] = field\n744 \n745     def _update_column_attribute_changed(self, column, idx, attr, old_value,\n746                                          new_value):\n747         \"\"\"\n748         Update how the data is formatted depending on changes to column\n749         attributes initiated by the user through the `Column` interface.\n750 \n751         Dispatches column attribute change notifications to individual methods\n752         for each attribute ``_update_column_<attr>``\n753         \"\"\"\n754 \n755         method_name = '_update_column_{0}'.format(attr)\n756         if hasattr(self, method_name):\n757             # Right now this is so we can be lazy and not implement updaters\n758             # for every attribute yet--some we may not need at all, TBD\n759             getattr(self, method_name)(column, idx, old_value, new_value)\n760 \n761     def _update_column_name(self, column, idx, old_name, name):\n762         \"\"\"Update the dtype field names when a column name is changed.\"\"\"\n763 \n764         dtype = self.dtype\n765         # Updating the names on the dtype should suffice\n766         dtype.names = dtype.names[:idx] + (name,) + dtype.names[idx + 1:]\n767 \n768     def _convert_x(self, field, recformat):\n769         \"\"\"Convert a raw table column to a bit array as specified by the\n770         FITS X format.\n771         \"\"\"\n772 \n773         dummy = np.zeros(self.shape + (recformat.repeat,), dtype=np.bool_)\n774         _unwrapx(field, dummy, recformat.repeat)\n775         return dummy\n776 \n777     def _convert_p(self, column, field, recformat):\n778         \"\"\"Convert a raw table column of FITS P or Q format descriptors\n779         to a VLA column with the array data returned from the heap.\n780         \"\"\"\n781 \n782         dummy = _VLF([None] * len(self), dtype=recformat.dtype)\n783         raw_data = self._get_raw_data()\n784 \n785         if raw_data is None:\n786             raise OSError(\n787                 \"Could not find heap data for the {!r} variable-length \"\n788                 \"array column.\".format(column.name))\n789 \n790         for idx in range(len(self)):\n791             offset = field[idx, 1] + self._heapoffset\n792             count = field[idx, 0]\n793 \n794             if recformat.dtype == 'a':\n795                 dt = np.dtype(recformat.dtype + str(1))\n796                 arr_len = count * dt.itemsize\n797                 da = raw_data[offset:offset + arr_len].view(dt)\n798                 da = np.char.array(da.view(dtype=dt), itemsize=count)\n799                 dummy[idx] = decode_ascii(da)\n800             else:\n801                 dt = np.dtype(recformat.dtype)\n802                 arr_len = count * dt.itemsize\n803                 dummy[idx] = raw_data[offset:offset + arr_len].view(dt)\n804                 dummy[idx].dtype = dummy[idx].dtype.newbyteorder('>')\n805                 # Each array in the field may now require additional\n806                 # scaling depending on the other scaling parameters\n807                 # TODO: The same scaling parameters apply to every\n808                 # array in the column so this is currently very slow; we\n809                 # really only need to check once whether any scaling will\n810                 # be necessary and skip this step if not\n811                 # TODO: Test that this works for X format; I don't think\n812                 # that it does--the recformat variable only applies to the P\n813                 # format not the X format\n814                 dummy[idx] = self._convert_other(column, dummy[idx],\n815                                                  recformat)\n816 \n817         return dummy\n818 \n819     def _convert_ascii(self, column, field):\n820         \"\"\"\n821         Special handling for ASCII table columns to convert columns containing\n822         numeric types to actual numeric arrays from the string representation.\n823         \"\"\"\n824 \n825         format = column.format\n826         recformat = ASCII2NUMPY[format[0]]\n827         # if the string = TNULL, return ASCIITNULL\n828         nullval = str(column.null).strip().encode('ascii')\n829         if len(nullval) > format.width:\n830             nullval = nullval[:format.width]\n831 \n832         # Before using .replace make sure that any trailing bytes in each\n833         # column are filled with spaces, and *not*, say, nulls; this causes\n834         # functions like replace to potentially leave gibberish bytes in the\n835         # array buffer.\n836         dummy = np.char.ljust(field, format.width)\n837         dummy = np.char.replace(dummy, encode_ascii('D'), encode_ascii('E'))\n838         null_fill = encode_ascii(str(ASCIITNULL).rjust(format.width))\n839 \n840         # Convert all fields equal to the TNULL value (nullval) to empty fields.\n841         # TODO: These fields really should be conerted to NaN or something else undefined.\n842         # Currently they are converted to empty fields, which are then set to zero.\n843         dummy = np.where(np.char.strip(dummy) == nullval, null_fill, dummy)\n844 \n845         # always replace empty fields, see https://github.com/astropy/astropy/pull/5394\n846         if nullval != b'':\n847             dummy = np.where(np.char.strip(dummy) == b'', null_fill, dummy)\n848 \n849         try:\n850             dummy = np.array(dummy, dtype=recformat)\n851         except ValueError as exc:\n852             indx = self.names.index(column.name)\n853             raise ValueError(\n854                 '{}; the header may be missing the necessary TNULL{} '\n855                 'keyword or the table contains invalid data'.format(\n856                     exc, indx + 1))\n857 \n858         return dummy\n859 \n860     def _convert_other(self, column, field, recformat):\n861         \"\"\"Perform conversions on any other fixed-width column data types.\n862 \n863         This may not perform any conversion at all if it's not necessary, in\n864         which case the original column array is returned.\n865         \"\"\"\n866 \n867         if isinstance(recformat, _FormatX):\n868             # special handling for the X format\n869             return self._convert_x(field, recformat)\n870 \n871         (_str, _bool, _number, _scale, _zero, bscale, bzero, dim) = \\\n872             self._get_scale_factors(column)\n873 \n874         indx = self.names.index(column.name)\n875 \n876         # ASCII table, convert strings to numbers\n877         # TODO:\n878         # For now, check that these are ASCII columns by checking the coldefs\n879         # type; in the future all columns (for binary tables, ASCII tables, or\n880         # otherwise) should \"know\" what type they are already and how to handle\n881         # converting their data from FITS format to native format and vice\n882         # versa...\n883         if not _str and isinstance(self._coldefs, _AsciiColDefs):\n884             field = self._convert_ascii(column, field)\n885 \n886         # Test that the dimensions given in dim are sensible; otherwise\n887         # display a warning and ignore them\n888         if dim:\n889             # See if the dimensions already match, if not, make sure the\n890             # number items will fit in the specified dimensions\n891             if field.ndim > 1:\n892                 actual_shape = field.shape[1:]\n893                 if _str:\n894                     actual_shape = actual_shape + (field.itemsize,)\n895             else:\n896                 actual_shape = field.shape[0]\n897 \n898             if dim == actual_shape:\n899                 # The array already has the correct dimensions, so we\n900                 # ignore dim and don't convert\n901                 dim = None\n902             else:\n903                 nitems = reduce(operator.mul, dim)\n904                 if _str:\n905                     actual_nitems = field.itemsize\n906                 elif len(field.shape) == 1:  # No repeat count in TFORMn, equivalent to 1\n907                     actual_nitems = 1\n908                 else:\n909                     actual_nitems = field.shape[1]\n910                 if nitems > actual_nitems:\n911                     warnings.warn(\n912                         'TDIM{} value {:d} does not fit with the size of '\n913                         'the array items ({:d}).  TDIM{:d} will be ignored.'\n914                         .format(indx + 1, self._coldefs[indx].dims,\n915                                 actual_nitems, indx + 1))\n916                     dim = None\n917 \n918         # further conversion for both ASCII and binary tables\n919         # For now we've made columns responsible for *knowing* whether their\n920         # data has been scaled, but we make the FITS_rec class responsible for\n921         # actually doing the scaling\n922         # TODO: This also needs to be fixed in the effort to make Columns\n923         # responsible for scaling their arrays to/from FITS native values\n924         if not column.ascii and column.format.p_format:\n925             format_code = column.format.p_format\n926         else:\n927             # TODO: Rather than having this if/else it might be nice if the\n928             # ColumnFormat class had an attribute guaranteed to give the format\n929             # of actual values in a column regardless of whether the true\n930             # format is something like P or Q\n931             format_code = column.format.format\n932 \n933         if (_number and (_scale or _zero) and not column._physical_values):\n934             # This is to handle pseudo unsigned ints in table columns\n935             # TODO: For now this only really works correctly for binary tables\n936             # Should it work for ASCII tables as well?\n937             if self._uint:\n938                 if bzero == 2**15 and format_code == 'I':\n939                     field = np.array(field, dtype=np.uint16)\n940                 elif bzero == 2**31 and format_code == 'J':\n941                     field = np.array(field, dtype=np.uint32)\n942                 elif bzero == 2**63 and format_code == 'K':\n943                     field = np.array(field, dtype=np.uint64)\n944                     bzero64 = np.uint64(2 ** 63)\n945                 else:\n946                     field = np.array(field, dtype=np.float64)\n947             else:\n948                 field = np.array(field, dtype=np.float64)\n949 \n950             if _scale:\n951                 np.multiply(field, bscale, field)\n952             if _zero:\n953                 if self._uint and format_code == 'K':\n954                     # There is a chance of overflow, so be careful\n955                     test_overflow = field.copy()\n956                     try:\n957                         test_overflow += bzero64\n958                     except OverflowError:\n959                         warnings.warn(\n960                             \"Overflow detected while applying TZERO{0:d}. \"\n961                             \"Returning unscaled data.\".format(indx + 1))\n962                     else:\n963                         field = test_overflow\n964                 else:\n965                     field += bzero\n966         elif _bool and field.dtype != bool:\n967             field = np.equal(field, ord('T'))\n968         elif _str:\n969             if not self._character_as_bytes:\n970                 with suppress(UnicodeDecodeError):\n971                     field = decode_ascii(field)\n972 \n973         if dim:\n974             # Apply the new field item dimensions\n975             nitems = reduce(operator.mul, dim)\n976             if field.ndim > 1:\n977                 field = field[:, :nitems]\n978             if _str:\n979                 fmt = field.dtype.char\n980                 dtype = ('|{}{}'.format(fmt, dim[-1]), dim[:-1])\n981                 field.dtype = dtype\n982             else:\n983                 field.shape = (field.shape[0],) + dim\n984 \n985         return field\n986 \n987     def _get_heap_data(self):\n988         \"\"\"\n989         Returns a pointer into the table's raw data to its heap (if present).\n990 \n991         This is returned as a numpy byte array.\n992         \"\"\"\n993 \n994         if self._heapsize:\n995             raw_data = self._get_raw_data().view(np.ubyte)\n996             heap_end = self._heapoffset + self._heapsize\n997             return raw_data[self._heapoffset:heap_end]\n998         else:\n999             return np.array([], dtype=np.ubyte)\n1000 \n1001     def _get_raw_data(self):\n1002         \"\"\"\n1003         Returns the base array of self that \"raw data array\" that is the\n1004         array in the format that it was first read from a file before it was\n1005         sliced or viewed as a different type in any way.\n1006 \n1007         This is determined by walking through the bases until finding one that\n1008         has at least the same number of bytes as self, plus the heapsize.  This\n1009         may be the immediate .base but is not always.  This is used primarily\n1010         for variable-length array support which needs to be able to find the\n1011         heap (the raw data *may* be larger than nbytes + heapsize if it\n1012         contains a gap or padding).\n1013 \n1014         May return ``None`` if no array resembling the \"raw data\" according to\n1015         the stated criteria can be found.\n1016         \"\"\"\n1017 \n1018         raw_data_bytes = self.nbytes + self._heapsize\n1019         base = self\n1020         while hasattr(base, 'base') and base.base is not None:\n1021             base = base.base\n1022             if hasattr(base, 'nbytes') and base.nbytes >= raw_data_bytes:\n1023                 return base\n1024 \n1025     def _get_scale_factors(self, column):\n1026         \"\"\"Get all the scaling flags and factors for one column.\"\"\"\n1027 \n1028         # TODO: Maybe this should be a method/property on Column?  Or maybe\n1029         # it's not really needed at all...\n1030         _str = column.format.format == 'A'\n1031         _bool = column.format.format == 'L'\n1032 \n1033         _number = not (_bool or _str)\n1034         bscale = column.bscale\n1035         bzero = column.bzero\n1036 \n1037         _scale = bscale not in ('', None, 1)\n1038         _zero = bzero not in ('', None, 0)\n1039 \n1040         # ensure bscale/bzero are numbers\n1041         if not _scale:\n1042             bscale = 1\n1043         if not _zero:\n1044             bzero = 0\n1045 \n1046         # column._dims gives a tuple, rather than column.dim which returns the\n1047         # original string format code from the FITS header...\n1048         dim = column._dims\n1049 \n1050         return (_str, _bool, _number, _scale, _zero, bscale, bzero, dim)\n1051 \n1052     def _scale_back(self, update_heap_pointers=True):\n1053         \"\"\"\n1054         Update the parent array, using the (latest) scaled array.\n1055 \n1056         If ``update_heap_pointers`` is `False`, this will leave all the heap\n1057         pointers in P/Q columns as they are verbatim--it only makes sense to do\n1058         this if there is already data on the heap and it can be guaranteed that\n1059         that data has not been modified, and there is not new data to add to\n1060         the heap.  Currently this is only used as an optimization for\n1061         CompImageHDU that does its own handling of the heap.\n1062         \"\"\"\n1063 \n1064         # Running total for the new heap size\n1065         heapsize = 0\n1066 \n1067         for indx, name in enumerate(self.dtype.names):\n1068             column = self._coldefs[indx]\n1069             recformat = column.format.recformat\n1070             raw_field = _get_recarray_field(self, indx)\n1071 \n1072             # add the location offset of the heap area for each\n1073             # variable length column\n1074             if isinstance(recformat, _FormatP):\n1075                 # Irritatingly, this can return a different dtype than just\n1076                 # doing np.dtype(recformat.dtype); but this returns the results\n1077                 # that we want.  For example if recformat.dtype is 'a' we want\n1078                 # an array of characters.\n1079                 dtype = np.array([], dtype=recformat.dtype).dtype\n1080 \n1081                 if update_heap_pointers and name in self._converted:\n1082                     # The VLA has potentially been updated, so we need to\n1083                     # update the array descriptors\n1084                     raw_field[:] = 0  # reset\n1085                     npts = [len(arr) for arr in self._converted[name]]\n1086 \n1087                     raw_field[:len(npts), 0] = npts\n1088                     raw_field[1:, 1] = (np.add.accumulate(raw_field[:-1, 0]) *\n1089                                         dtype.itemsize)\n1090                     raw_field[:, 1][:] += heapsize\n1091 \n1092                 heapsize += raw_field[:, 0].sum() * dtype.itemsize\n1093                 # Even if this VLA has not been read or updated, we need to\n1094                 # include the size of its constituent arrays in the heap size\n1095                 # total\n1096 \n1097             if isinstance(recformat, _FormatX) and name in self._converted:\n1098                 _wrapx(self._converted[name], raw_field, recformat.repeat)\n1099                 continue\n1100 \n1101             _str, _bool, _number, _scale, _zero, bscale, bzero, _ = \\\n1102                 self._get_scale_factors(column)\n1103 \n1104             field = self._converted.get(name, raw_field)\n1105 \n1106             # conversion for both ASCII and binary tables\n1107             if _number or _str:\n1108                 if _number and (_scale or _zero) and column._physical_values:\n1109                     dummy = field.copy()\n1110                     if _zero:\n1111                         dummy -= bzero\n1112                     if _scale:\n1113                         dummy /= bscale\n1114                     # This will set the raw values in the recarray back to\n1115                     # their non-physical storage values, so the column should\n1116                     # be mark is not scaled\n1117                     column._physical_values = False\n1118                 elif _str or isinstance(self._coldefs, _AsciiColDefs):\n1119                     dummy = field\n1120                 else:\n1121                     continue\n1122 \n1123                 # ASCII table, convert numbers to strings\n1124                 if isinstance(self._coldefs, _AsciiColDefs):\n1125                     self._scale_back_ascii(indx, dummy, raw_field)\n1126                 # binary table string column\n1127                 elif isinstance(raw_field, chararray.chararray):\n1128                     self._scale_back_strings(indx, dummy, raw_field)\n1129                 # all other binary table columns\n1130                 else:\n1131                     if len(raw_field) and isinstance(raw_field[0],\n1132                                                      np.integer):\n1133                         dummy = np.around(dummy)\n1134 \n1135                     if raw_field.shape == dummy.shape:\n1136                         raw_field[:] = dummy\n1137                     else:\n1138                         # Reshaping the data is necessary in cases where the\n1139                         # TDIMn keyword was used to shape a column's entries\n1140                         # into arrays\n1141                         raw_field[:] = dummy.ravel().view(raw_field.dtype)\n1142 \n1143                 del dummy\n1144 \n1145             # ASCII table does not have Boolean type\n1146             elif _bool and name in self._converted:\n1147                 choices = (np.array([ord('F')], dtype=np.int8)[0],\n1148                            np.array([ord('T')], dtype=np.int8)[0])\n1149                 raw_field[:] = np.choose(field, choices)\n1150 \n1151         # Store the updated heapsize\n1152         self._heapsize = heapsize\n1153 \n1154     def _scale_back_strings(self, col_idx, input_field, output_field):\n1155         # There are a few possibilities this has to be able to handle properly\n1156         # The input_field, which comes from the _converted column is of dtype\n1157         # 'Un' so that elements read out of the array are normal str\n1158         # objects (i.e. unicode strings)\n1159         #\n1160         # At the other end the *output_field* may also be of type 'S' or of\n1161         # type 'U'.  It will *usually* be of type 'S' because when reading\n1162         # an existing FITS table the raw data is just ASCII strings, and\n1163         # represented in Numpy as an S array.  However, when a user creates\n1164         # a new table from scratch, they *might* pass in a column containing\n1165         # unicode strings (dtype 'U').  Therefore the output_field of the\n1166         # raw array is actually a unicode array.  But we still want to make\n1167         # sure the data is encodable as ASCII.  Later when we write out the\n1168         # array we use, in the dtype 'U' case, a different write routine\n1169         # that writes row by row and encodes any 'U' columns to ASCII.\n1170 \n1171         # If the output_field is non-ASCII we will worry about ASCII encoding\n1172         # later when writing; otherwise we can do it right here\n1173         if input_field.dtype.kind == 'U' and output_field.dtype.kind == 'S':\n1174             try:\n1175                 _ascii_encode(input_field, out=output_field)\n1176             except _UnicodeArrayEncodeError as exc:\n1177                 raise ValueError(\n1178                     \"Could not save column '{0}': Contains characters that \"\n1179                     \"cannot be encoded as ASCII as required by FITS, starting \"\n1180                     \"at the index {1!r} of the column, and the index {2} of \"\n1181                     \"the string at that location.\".format(\n1182                         self._coldefs[col_idx].name,\n1183                         exc.index[0] if len(exc.index) == 1 else exc.index,\n1184                         exc.start))\n1185         else:\n1186             # Otherwise go ahead and do a direct copy into--if both are type\n1187             # 'U' we'll handle encoding later\n1188             input_field = input_field.flatten().view(output_field.dtype)\n1189             output_field.flat[:] = input_field\n1190 \n1191         # Ensure that blanks at the end of each string are\n1192         # converted to nulls instead of spaces, see Trac #15\n1193         # and #111\n1194         _rstrip_inplace(output_field)\n1195 \n1196     def _scale_back_ascii(self, col_idx, input_field, output_field):\n1197         \"\"\"\n1198         Convert internal array values back to ASCII table representation.\n1199 \n1200         The ``input_field`` is the internal representation of the values, and\n1201         the ``output_field`` is the character array representing the ASCII\n1202         output that will be written.\n1203         \"\"\"\n1204 \n1205         starts = self._coldefs.starts[:]\n1206         spans = self._coldefs.spans\n1207         format = self._coldefs[col_idx].format\n1208 \n1209         # The the index of the \"end\" column of the record, beyond\n1210         # which we can't write\n1211         end = super().field(-1).itemsize\n1212         starts.append(end + starts[-1])\n1213 \n1214         if col_idx > 0:\n1215             lead = starts[col_idx] - starts[col_idx - 1] - spans[col_idx - 1]\n1216         else:\n1217             lead = 0\n1218 \n1219         if lead < 0:\n1220             warnings.warn('Column {!r} starting point overlaps the previous '\n1221                           'column.'.format(col_idx + 1))\n1222 \n1223         trail = starts[col_idx + 1] - starts[col_idx] - spans[col_idx]\n1224 \n1225         if trail < 0:\n1226             warnings.warn('Column {!r} ending point overlaps the next '\n1227                           'column.'.format(col_idx + 1))\n1228 \n1229         # TODO: It would be nice if these string column formatting\n1230         # details were left to a specialized class, as is the case\n1231         # with FormatX and FormatP\n1232         if 'A' in format:\n1233             _pc = '{:'\n1234         else:\n1235             _pc = '{:>'\n1236 \n1237         fmt = ''.join([_pc, format[1:], ASCII2STR[format[0]], '}',\n1238                        (' ' * trail)])\n1239 \n1240         # Even if the format precision is 0, we should output a decimal point\n1241         # as long as there is space to do so--not including a decimal point in\n1242         # a float value is discouraged by the FITS Standard\n1243         trailing_decimal = (format.precision == 0 and\n1244                             format.format in ('F', 'E', 'D'))\n1245 \n1246         # not using numarray.strings's num2char because the\n1247         # result is not allowed to expand (as C/Python does).\n1248         for jdx, value in enumerate(input_field):\n1249             value = fmt.format(value)\n1250             if len(value) > starts[col_idx + 1] - starts[col_idx]:\n1251                 raise ValueError(\n1252                     \"Value {!r} does not fit into the output's itemsize of \"\n1253                     \"{}.\".format(value, spans[col_idx]))\n1254 \n1255             if trailing_decimal and value[0] == ' ':\n1256                 # We have some extra space in the field for the trailing\n1257                 # decimal point\n1258                 value = value[1:] + '.'\n1259 \n1260             output_field[jdx] = value\n1261 \n1262         # Replace exponent separator in floating point numbers\n1263         if 'D' in format:\n1264             output_field.replace(encode_ascii('E'), encode_ascii('D'))\n1265 \n1266 \n1267 def _get_recarray_field(array, key):\n1268     \"\"\"\n1269     Compatibility function for using the recarray base class's field method.\n1270     This incorporates the legacy functionality of returning string arrays as\n1271     Numeric-style chararray objects.\n1272     \"\"\"\n1273 \n1274     # Numpy >= 1.10.dev recarray no longer returns chararrays for strings\n1275     # This is currently needed for backwards-compatibility and for\n1276     # automatic truncation of trailing whitespace\n1277     field = np.recarray.field(array, key)\n1278     if (field.dtype.char in ('S', 'U') and\n1279             not isinstance(field, chararray.chararray)):\n1280         field = field.view(chararray.chararray)\n1281     return field\n1282 \n1283 \n1284 class _UnicodeArrayEncodeError(UnicodeEncodeError):\n1285     def __init__(self, encoding, object_, start, end, reason, index):\n1286         super().__init__(encoding, object_, start, end, reason)\n1287         self.index = index\n1288 \n1289 \n1290 def _ascii_encode(inarray, out=None):\n1291     \"\"\"\n1292     Takes a unicode array and fills the output string array with the ASCII\n1293     encodings (if possible) of the elements of the input array.  The two arrays\n1294     must be the same size (though not necessarily the same shape).\n1295 \n1296     This is like an inplace version of `np.char.encode` though simpler since\n1297     it's only limited to ASCII, and hence the size of each character is\n1298     guaranteed to be 1 byte.\n1299 \n1300     If any strings are non-ASCII an UnicodeArrayEncodeError is raised--this is\n1301     just a `UnicodeEncodeError` with an additional attribute for the index of\n1302     the item that couldn't be encoded.\n1303     \"\"\"\n1304 \n1305     out_dtype = np.dtype(('S{0}'.format(inarray.dtype.itemsize // 4),\n1306                          inarray.dtype.shape))\n1307     if out is not None:\n1308         out = out.view(out_dtype)\n1309 \n1310     op_dtypes = [inarray.dtype, out_dtype]\n1311     op_flags = [['readonly'], ['writeonly', 'allocate']]\n1312     it = np.nditer([inarray, out], op_dtypes=op_dtypes,\n1313                    op_flags=op_flags, flags=['zerosize_ok'])\n1314 \n1315     try:\n1316         for initem, outitem in it:\n1317             outitem[...] = initem.item().encode('ascii')\n1318     except UnicodeEncodeError as exc:\n1319         index = np.unravel_index(it.iterindex, inarray.shape)\n1320         raise _UnicodeArrayEncodeError(*(exc.args + (index,)))\n1321 \n1322     return it.operands[1]\n1323 \n1324 \n1325 def _has_unicode_fields(array):\n1326     \"\"\"\n1327     Returns True if any fields in a structured array have Unicode dtype.\n1328     \"\"\"\n1329 \n1330     dtypes = (d[0] for d in array.dtype.fields.values())\n1331     return any(d.kind == 'U' for d in dtypes)\n1332 \n[end of astropy/io/fits/fitsrec.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n"
}